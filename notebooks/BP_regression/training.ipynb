{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score\n",
    "# from resnet1d import ResNet1D\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# laveling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data = np.load('../../data/processed/BP_npy/PulseDB/train_sbp_2.npy')\n",
    "\n",
    "# ラベリング関数の定義\n",
    "def label_blood_pressure(bp):\n",
    "    if bp <= 100:\n",
    "        return 0    # 正常血圧\n",
    "    elif 100 < bp < 120:\n",
    "        return 1    # 正常血圧\n",
    "    elif 120 < bp < 140:\n",
    "        return 2    # 正常高値血圧\n",
    "    else:\n",
    "        return 3    # 高血圧\n",
    "\n",
    "# ベクトル化した関数を作成\n",
    "vectorized_label = np.vectorize(label_blood_pressure)\n",
    "\n",
    "# データ全体にラベリングを適用\n",
    "bp_labels = vectorized_label(bp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラベルの分布:\n",
      "ラベル 0: 171782件\n",
      "ラベル 3: 140660件\n"
     ]
    }
   ],
   "source": [
    "print(\"ラベルの分布:\")\n",
    "unique, counts = np.unique(bp_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"ラベル {label}: {count}件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data/processed/BP_npy/PulseDB/test_sbp_4labels.npy', bp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2値\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 111600 is out of bounds for axis 0 with size 111600",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 対応するデータの抽出\u001b[39;00m\n\u001b[0;32m      9\u001b[0m original_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/processed/BP_npy/PulseDB/test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     11\u001b[0m filtered_labels \u001b[38;5;241m=\u001b[39m labels[selected_indices]\n\u001b[0;32m     12\u001b[0m filtered_labels[filtered_labels\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 111600 is out of bounds for axis 0 with size 111600"
     ]
    }
   ],
   "source": [
    "# ラベルデータの読み込み\n",
    "labels = np.load('../../data/processed/BP_npy/PulseDB/test_sbp_4labels.npy')\n",
    "\n",
    "# ラベルが0または3のインデックスを抽出\n",
    "mask = (labels == 0) | (labels == 3)\n",
    "selected_indices = np.where(mask)[0]\n",
    "\n",
    "# 対応するデータの抽出\n",
    "original_data = np.load('../../data/processed/BP_npy/PulseDB/test.npy')\n",
    "filtered_data = original_data[selected_indices]\n",
    "filtered_labels = labels[selected_indices]\n",
    "filtered_labels[filtered_labels==3]=1\n",
    "# 確認\n",
    "print(f\"元のデータ数: {len(labels)}\")\n",
    "print(f\"フィルタ後のデータ数: {len(filtered_labels)}\")\n",
    "print(\"ラベルの分布:\")\n",
    "unique, counts = np.unique(filtered_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"ラベル {label}: {count}件\")\n",
    "\n",
    "# 必要に応じて新しいファイルとして保存\n",
    "np.save('../../data/processed/BP_npy/PulseDB/test_2.npy', filtered_data)\n",
    "np.save('../../data/processed/BP_npy/PulseDB/test_sbp_2labels.npy', filtered_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cv\n",
    "cv[0][:5] : train  \n",
    "cv[1][:5] : val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(902160,)\n"
     ]
    }
   ],
   "source": [
    "cv_idx_path = r\"../../data/processed/BP_npy/PulseDB/cv_5fold.pkl\"\n",
    "with open(cv_idx_path, \"rb\") as f:\n",
    "    cv_idx = pickle.load(f)\n",
    "labels = np.load('../../data/processed/BP_npy/PulseDB/train_sbp_4labels.npy')\n",
    "train_sbp = []\n",
    "val_sbp = []\n",
    "train_idx = []\n",
    "val_idx = []\n",
    "for fold in range(5):\n",
    "    train_ = labels[cv_idx[0][fold]]\n",
    "    idx_ = np.where((train_==0)|(train_==3))\n",
    "    train_idx.append(idx_)\n",
    "    train_ = train_[idx_]\n",
    "    # train_[train_==0]=0\n",
    "    train_[train_==3]=1\n",
    "    train_sbp.append(train_)\n",
    "    val_ = labels[cv_idx[1][fold]]\n",
    "    idx_ = np.where((val_==0)|(val_==3))\n",
    "    val_idx.append(idx_)\n",
    "    val_ = val_[idx_]\n",
    "    # val_[val_==0]=0\n",
    "    val_[val_==3]=1\n",
    "    val_sbp.append(val_)\n",
    "sbp_2labels_cv = [train_sbp,val_sbp]\n",
    "ppgidx_2labels_cv = [train_idx,val_idx]\n",
    "with open('../../data/processed/BP_npy/PulseDB/ppgidx_2labels_cv.pkl','wb') as f:\n",
    "    pickle.dump(ppgidx_2labels_cv,f)\n",
    "with open('../../data/processed/BP_npy/PulseDB/sbp_2labels_cv.pkl','wb') as f:\n",
    "    pickle.dump(sbp_2labels_cv,f)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のデータ数: 902160\n",
      "フィルタ後のデータ数: 312442\n",
      "ラベルの分布:\n",
      "ラベル 0: 171782件\n",
      "ラベル 1: 140660件\n"
     ]
    }
   ],
   "source": [
    "# ラベルデータの読み込み\n",
    "labels = np.load('../../data/processed/BP_npy/PulseDB/train_sbp_4labels.npy')\n",
    "\n",
    "# ラベルが0または3のインデックスを抽出\n",
    "mask = (labels == 0) | (labels == 3)\n",
    "selected_indices = np.where(mask)[0]\n",
    "\n",
    "# 対応するデータの抽出\n",
    "original_data = np.load('../../data/processed/BP_npy/PulseDB/train.npy')\n",
    "filtered_data = original_data[selected_indices]\n",
    "filtered_labels = labels[selected_indices]\n",
    "filtered_labels[filtered_labels==3]=1\n",
    "# 確認\n",
    "print(f\"元のデータ数: {len(labels)}\")\n",
    "print(f\"フィルタ後のデータ数: {len(filtered_labels)}\")\n",
    "print(\"ラベルの分布:\")\n",
    "unique, counts = np.unique(filtered_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"ラベル {label}: {count}件\")\n",
    "\n",
    "# 必要に応じて新しいファイルとして保存\n",
    "np.save('../../data/processed/BP_npy/PulseDB/train_2.npy', filtered_data)\n",
    "np.save('../../data/processed/BP_npy/PulseDB/train_sbp_2labels.npy', filtered_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# laveling retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(902160,)\n"
     ]
    }
   ],
   "source": [
    "bp_data = np.load('../../data/processed/BP_npy/PulseDB/train_sbp.npy')\n",
    "print(bp_data.shape)\n",
    "# ラベリング関数の定義\n",
    "def label_blood_pressure(bp):\n",
    "    if bp <= 100:\n",
    "        return 0    # 正常血圧\n",
    "    elif 100 < bp < 120:\n",
    "        return 1    # 正常血圧\n",
    "    elif 120 < bp < 140:\n",
    "        return 2    # 正常高値血圧\n",
    "    else:\n",
    "        return 3    # 高血圧\n",
    "\n",
    "# ベクトル化した関数を作成\n",
    "vectorized_label = np.vectorize(label_blood_pressure)\n",
    "\n",
    "# データ全体にラベリングを適用\n",
    "bp_labels = vectorized_label(bp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のデータ数: 902160\n",
      "ラベルの分布:\n",
      "ラベル 0: 171782件\n",
      "ラベル 1: 336150件\n",
      "ラベル 2: 253568件\n",
      "ラベル 3: 140660件\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "print(f\"元のデータ数: {len(bp_labels)}\")\n",
    "# print(f\"フィルタ後のデータ数: {len(filtered_labels)}\")\n",
    "print(\"ラベルの分布:\")\n",
    "unique, counts = np.unique(bp_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"ラベル {label}: {count}件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data/processed/BP_npy/PulseDB/train_sbp_labels.npy',bp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のデータ数: 902160\n",
      "フィルタ後のデータ数: 312442\n",
      "ラベルの分布:\n",
      "ラベル 0: 171782件\n",
      "ラベル 1: 140660件\n"
     ]
    }
   ],
   "source": [
    "labels = np.load('../../data/processed/BP_npy/PulseDB/train_sbp_labels.npy')\n",
    "\n",
    "# ラベルが0または3のインデックスを抽出\n",
    "mask = (labels == 0) | (labels == 3)\n",
    "selected_indices = np.where(mask)[0]\n",
    "\n",
    "# 対応するデータの抽出\n",
    "original_data = np.load('../../data/processed/BP_npy/PulseDB/train_raw.npy')\n",
    "filtered_data = original_data[selected_indices]\n",
    "filtered_labels = labels[selected_indices]\n",
    "filtered_labels[filtered_labels==3]=1\n",
    "# 確認\n",
    "print(f\"元のデータ数: {len(labels)}\")\n",
    "print(f\"フィルタ後のデータ数: {len(filtered_labels)}\")\n",
    "print(\"ラベルの分布:\")\n",
    "unique, counts = np.unique(filtered_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"ラベル {label}: {count}件\")\n",
    "\n",
    "# 必要に応じて新しいファイルとして保存\n",
    "np.save('../../data/processed/BP_npy/PulseDB/train_2.npy', filtered_data)\n",
    "np.save('../../data/processed/BP_npy/PulseDB/train_sbp_2labels.npy', filtered_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のデータ数: 111600\n",
      "ラベルの分布:\n",
      "ラベル 0: 20674件\n",
      "ラベル 1: 41043件\n",
      "ラベル 2: 32447件\n",
      "ラベル 3: 17436件\n"
     ]
    }
   ],
   "source": [
    "bp_data = np.load('../../data/processed/BP_npy/PulseDB/test_sbp.npy')\n",
    "vectorized_label = np.vectorize(label_blood_pressure)\n",
    "# データ全体にラベリングを適用\n",
    "bp_labels = vectorized_label(bp_data)\n",
    "# 確認\n",
    "print(f\"元のデータ数: {len(bp_labels)}\")\n",
    "# print(f\"フィルタ後のデータ数: {len(filtered_labels)}\")\n",
    "print(\"ラベルの分布:\")\n",
    "unique, counts = np.unique(bp_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"ラベル {label}: {count}件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data/processed/BP_npy/PulseDB/test_sbp_labels.npy',bp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のデータ数: 111600\n",
      "フィルタ後のデータ数: 38110\n",
      "ラベルの分布:\n",
      "ラベル 0: 20674件\n",
      "ラベル 1: 17436件\n"
     ]
    }
   ],
   "source": [
    "labels = np.load('../../data/processed/BP_npy/PulseDB/test_sbp_labels.npy')\n",
    "\n",
    "# ラベルが0または3のインデックスを抽出\n",
    "mask = (labels == 0) | (labels == 3)\n",
    "selected_indices = np.where(mask)[0]\n",
    "\n",
    "# 対応するデータの抽出\n",
    "original_data = np.load('../../data/processed/BP_npy/PulseDB/test_raw.npy')\n",
    "filtered_data = original_data[selected_indices]\n",
    "filtered_labels = labels[selected_indices]\n",
    "filtered_labels[filtered_labels==3]=1\n",
    "# 確認\n",
    "print(f\"元のデータ数: {len(labels)}\")\n",
    "print(f\"フィルタ後のデータ数: {len(filtered_labels)}\")\n",
    "print(\"ラベルの分布:\")\n",
    "unique, counts = np.unique(filtered_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"ラベル {label}: {count}件\")\n",
    "\n",
    "# 必要に応じて新しいファイルとして保存\n",
    "np.save('../../data/processed/BP_npy/PulseDB/test_2.npy', filtered_data)\n",
    "np.save('../../data/processed/BP_npy/PulseDB/test_sbp_2labels.npy', filtered_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cv laveling retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(902160,)\n"
     ]
    }
   ],
   "source": [
    "cv_idx_path = r\"../../data/processed/BP_npy/PulseDB/cv_5fold_2labels.pkl\"\n",
    "with open(cv_idx_path, \"rb\") as f:\n",
    "    cv_idx = pickle.load(f)\n",
    "labels = np.load('../../data/processed/BP_npy/PulseDB/train_sbp_2labels.npy')\n",
    "train_sbp = []\n",
    "val_sbp = []\n",
    "train_idx = []\n",
    "val_idx = []\n",
    "for fold in range(5):\n",
    "    train_ = labels[cv_idx[0][fold]]\n",
    "    idx_ = np.where((train_==0)|(train_==3))\n",
    "    train_idx.append(idx_)\n",
    "    train_ = train_[idx_]\n",
    "    # train_[train_==0]=0\n",
    "    train_[train_==3]=1\n",
    "    train_sbp.append(train_)\n",
    "    val_ = labels[cv_idx[1][fold]]\n",
    "    idx_ = np.where((val_==0)|(val_==3))\n",
    "    val_idx.append(idx_)\n",
    "    val_ = val_[idx_]\n",
    "    # val_[val_==0]=0\n",
    "    val_[val_==3]=1\n",
    "    val_sbp.append(val_)\n",
    "sbp_2labels_cv = [train_sbp,val_sbp]\n",
    "ppgidx_2labels_cv = [train_idx,val_idx]\n",
    "with open('../../data/processed/BP_npy/PulseDB/ppgidx_2labels_cv.pkl','wb') as f:\n",
    "    pickle.dump(ppgidx_2labels_cv,f)\n",
    "with open('../../data/processed/BP_npy/PulseDB/sbp_2labels_cv.pkl','wb') as f:\n",
    "    pickle.dump(sbp_2labels_cv,f)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BPDataset(Dataset):\n",
    "#     def __init__(self, data_dir,train=True):\n",
    "#         # Load data\n",
    "#         if train:\n",
    "#             self.x = np.load(f'{data_dir}/train.npy')[:, 1, :].reshape(-1,1,1250)  # Shape: (-1, 1250)\n",
    "#             self.y = np.load(f'{data_dir}/train_sbp_4labels.npy')  # Shape: (-1,)\n",
    "#             print(self.x.shape,self.y.shape)\n",
    "#             # self.x = np.load(f'{data_dir}/train_2.npy')[:, 1, :].reshape(-1,1,1250)  # Shape: (-1, 1250)\n",
    "#             # self.y = np.load(f'{data_dir}/train_sbp_2labels.npy')  # Shape: (-1,)\n",
    "#         else:\n",
    "#             self.x = np.load(f'{data_dir}/test.npy')[:, 1, :].reshape(-1,1,1250)  # Shape: (-1, 1250)\n",
    "#             self.y = np.load(f'{data_dir}/test_sbp_4labels.npy')  # Shape: (-1,)\n",
    "#             # self.x = np.load(f'{data_dir}/test_2.npy')[:, 1, :].reshape(-1,1,1250)  # Shape: (-1, 1250)\n",
    "#             # self.y = np.load(f'{data_dir}/test_sbp_2labels.npy')  # Shape: (-1,)\n",
    "        \n",
    "#         # Convert to torch tensors\n",
    "#         self.x = torch.FloatTensor(self.x)\n",
    "#         self.y = torch.LongTensor(self.y)\n",
    "#         print(self.x.shape,self.y.shape)\n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.x[idx], self.y[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPDataset(Dataset):\n",
    "    def __init__(self, data_dir,cv=False,fold=None,train=True):\n",
    "        # Load data\n",
    "        if train:\n",
    "            if cv:\n",
    "                self.x = np.load(f'{data_dir}/train_2.npy')[:, 1, :].reshape(-1,1,1250)\n",
    "                self.y = np.load(f'{data_dir}/train_sbp_2labels.npy')\n",
    "                with open(f'{data_dir}/cv_5fold_2labels.pkl','rb') as f:\n",
    "                    file = pickle.load(f)\n",
    "                    self.x = self.x[file[0][fold]]\n",
    "                    self.y = self.y[file[0][fold]]\n",
    "                # with open(f'{data_dir}/ppgidx_2labels_cv.pkl','rb') as f:\n",
    "                #     ppgidx_2labels_cv = pickle.load(f)\n",
    "                # self.x = self.x[ppgidx_2labels_cv[0][fold]]\n",
    "                # with open(f'{data_dir}/sbp_2labels_cv.pkl','rb') as f:\n",
    "                #     sbp_4labels_cv = pickle.load(f)\n",
    "            else:\n",
    "                self.x = np.load(f'{data_dir}/train_2.npy')[:, 1, :].reshape(-1,1,1250)  # Shape: (-1, 1250)\n",
    "                self.y = np.load(f'{data_dir}/train_sbp_2labels.npy')  # Shape: (-1,)\n",
    "            print(self.x.shape,self.y.shape)\n",
    "            # self.x = np.load(f'{data_dir}/train_2.npy')[:, 1, :].reshape(-1,1,1250)  # Shape: (-1, 1250)\n",
    "            # self.y = np.load(f'{data_dir}/train_sbp_2labels.npy')  # Shape: (-1,)\n",
    "        else:\n",
    "            if cv:\n",
    "                self.x = np.load(f'{data_dir}/train_2.npy')[:, 1, :].reshape(-1,1,1250) # Shape: (-1, 1250)\n",
    "                self.y = np.load(f'{data_dir}/train_sbp_2labels.npy')\n",
    "                with open(f'{data_dir}/cv_5fold_2labels.pkl','rb') as f:\n",
    "                    file = pickle.load(f)\n",
    "                    self.x = self.x[file[1][fold]]\n",
    "                    self.y = self.y[file[1][fold]]\n",
    "                # with open(f'{data_dir}/ppgidx_2labels_cv.pkl','rb') as f:\n",
    "                #     ppgidx_2labels_cv = pickle.load(f)\n",
    "                # self.x = self.x[ppgidx_2labels_cv[1][fold]]\n",
    "                # with open(f'{data_dir}/sbp_2labels_cv.pkl','rb') as f:\n",
    "                #     sbp_4labels_cv = pickle.load(f)\n",
    "                # self.y = sbp_4labels_cv[1][fold]  # Shape: (-1,)\n",
    "            else:\n",
    "                self.x = np.load(f'{data_dir}/test_2.npy')[:, 1, :].reshape(-1,1,1250)  # Shape: (-1, 1250)\n",
    "                self.y = np.load(f'{data_dir}/test_sbp_2labels.npy')  # Shape: (-1,)\n",
    "            # self.x = np.load(f'{data_dir}/test_2.npy')[:, 1, :].reshape(-1,1,1250)  # Shape: (-1, 1250)\n",
    "            # self.y = np.load(f'{data_dir}/test_sbp_2labels.npy')  # Shape: (-1,)\n",
    "        # Convert to torch tensors\n",
    "        self.x = torch.FloatTensor(self.x)\n",
    "        self.y = torch.LongTensor(self.y)\n",
    "        print(self.x.shape,self.y.shape)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPDataset_Regr(Dataset):\n",
    "    def __init__(self, data_dir,cv=None,train=True):\n",
    "        # Load data\n",
    "        if train:\n",
    "            self.x = np.load(f'{data_dir}/train.npy')[:, 1, :].reshape(-1,1,1250)  # Shape: (-1, 1250)\n",
    "            self.sbp = np.load(f'{data_dir}/train_sbp.npy')  # Shape: (-1,)\n",
    "            self.dbp = np.load(f'{data_dir}/train_dbp.npy')  # Shape: (-1,)\n",
    "        else:\n",
    "            if cv is not None:\n",
    "                self.x = np.load(f'{data_dir}/train.npy')[:, 1, :].reshape(-1,1,1250)  # Shape: (-1, 1250)\n",
    "                self.sbp = np.load(f'{data_dir}/train_sbp.npy')  # Shape: (-1,)\n",
    "                self.dbp = np.load(f'{data_dir}/train_dbp.npy')\n",
    "\n",
    "            else:\n",
    "                self.x = np.load(f'{data_dir}/test.npy')[:, 1, :].reshape(-1,1,1250)  # Shape: (-1, 1250)\n",
    "                self.sbp = np.load(f'{data_dir}/test_sbp.npy')  # Shape: (-1,)\n",
    "                self.dbp = np.load(f'{data_dir}/test_dbp.npy')  # Shape: (-1,)\n",
    "        if cv is not None:\n",
    "            self.x = self.x[cv]\n",
    "            self.sbp = self.sbp[cv]\n",
    "            self.dbp = self.dbp[cv]\n",
    "        scale = np.load(f'{data_dir}/scale_train.npy')\n",
    "        self.sbp = self.sbp * scale[0,1]- scale[0,0]\n",
    "        self.dbp = self.dbp * scale[0,1] - scale[0,0]\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        self.x = torch.FloatTensor(self.x)\n",
    "        self.sbp = torch.FloatTensor(self.sbp)\n",
    "        self.dbp = torch.FloatTensor(self.dbp)\n",
    "        print(self.x.shape,self.sbp.shape,self.dbp.shape)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.sbp[idx],self.dbp[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"earlystoppingクラス\"\"\"\n",
    "\n",
    "    def __init__(self, path, patience=5, verbose=False):\n",
    "        \"\"\"引数：最小値の非更新数カウンタ、表示設定、モデル格納path\"\"\"\n",
    "\n",
    "        self.patience = patience    #設定ストップカウンタ\n",
    "        self.verbose = verbose      #表示の有無\n",
    "        self.counter = 0            #現在のカウンタ値\n",
    "        self.best_score = None      #ベストスコア\n",
    "        self.early_stop = False     #ストップフラグ\n",
    "        # self.val_loss_min = np.Inf   #前回のベストスコア記憶用\n",
    "        \n",
    "        self.path = path             #ベストモデル格納path\n",
    "        os.makedirs(os.path.dirname(self.path),exist_ok=True)\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        特殊(call)メソッド\n",
    "        実際に学習ループ内で最小lossを更新したか否かを計算させる部分\n",
    "        \"\"\"\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:  #1Epoch目の処理\n",
    "            self.best_score = score\n",
    "            self.checkpoint(score, model)  #記録後にモデルを保存してスコア表示する\n",
    "        elif score > self.best_score:  # ベストスコアを更新できなかった場合\n",
    "            self.counter += 1   #ストップカウンタを+1\n",
    "            if self.verbose:  #表示を有効にした場合は経過を表示\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #現在のカウンタを表示する\n",
    "                print(f\"the best of loss: {self.best_score:.5f}\")\n",
    "            if self.counter >= self.patience:  #設定カウントを上回ったらストップフラグをTrueに変更\n",
    "                self.early_stop = True\n",
    "        else:  #ベストスコアを更新した場合\n",
    "            self.checkpoint(score, model)  #モデルを保存してスコア表示\n",
    "            self.counter = 0  #ストップカウンタリセット\n",
    "\n",
    "    def checkpoint(self, score, model):\n",
    "        '''ベストスコア更新時に実行されるチェックポイント関数'''\n",
    "        if self.verbose:  #表示を有効にした場合は、前回のベストスコアからどれだけ更新したか？を表示\n",
    "            print(f'Validation loss decreased ({self.best_score:.5f} --> {score:.5f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)  #ベストモデルを指定したpathに保存\n",
    "        self.best_score = score #その時のlossを記録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_warning(config):\n",
    "    # 'output_path'が存在する場合\n",
    "    if os.path.exists(config[\"output_path\"]):\n",
    "        # ユーザーに確認を求める（enterを押したらyes）\n",
    "        confirmation = input(f\"Warning: The output path {config['output_path']} already exists. Do you want to continue? Press Enter to continue: \").strip().lower()\n",
    "        \n",
    "        # 入力が空（Enter）が押された場合は続行、それ以外はエラー\n",
    "        if confirmation == \"\" or confirmation == \"y\":\n",
    "            print(\"Proceeding with the operation...\")\n",
    "        else:\n",
    "            raise ValueError(\"Operation aborted by the user.\")\n",
    "    else:\n",
    "        print(f\"The output path {config['output_path']} does not exist. Proceeding...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_figure(gt,pred):\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.plot(gt,label=\"true\")  \n",
    "    ax.plot(gt,label=\"pred\")\n",
    "    ax.legend()\n",
    "    return fig\n",
    "def log_img(gt,pred):\n",
    "    gt, pred = gt[0].detach().clone().cpu().numpy(), pred[0].detach().clone().cpu().numpy()\n",
    "    figure = create_figure(gt,pred)\n",
    "    img = wandb.Image(figure)\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    return img\n",
    "def compute_class_weights(labels):\n",
    "# クラスごとのサンプル数をカウント\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    total_samples = len(labels)\n",
    "    \n",
    "    # クラスの重みを計算 (サンプル数の逆数で重み付け)\n",
    "    weights = total_samples / (len(unique) * counts)\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "# モデルの評価指標\n",
    "def calculate_metrics(y_true, y_pred,classes=2):\n",
    "    # 正解率\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    \n",
    "    # クラスごとの精度を計算\n",
    "    class_accuracies = []\n",
    "    for cls in range(classes):\n",
    "        mask = (y_true == cls)\n",
    "        if np.sum(mask) > 0:\n",
    "            class_acc = np.sum((y_true == y_pred) & mask) / np.sum(mask)\n",
    "            class_accuracies.append(class_acc)\n",
    "        else:\n",
    "            raise ValueError(f\"Class {cls} has no samples.\")\n",
    "    \n",
    "    # マクロ平均F1スコア\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    return accuracy, class_accuracies, f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312442, 1, 1250) (312442,)\n",
      "torch.Size([312442, 1, 1250]) torch.Size([312442])\n",
      "torch.Size([38110, 1, 1250]) torch.Size([38110])\n",
      "Training data:\n",
      "x shape: torch.Size([1, 1250]), x dtype: torch.float32\n",
      "y shape: torch.Size([]), y dtype: torch.int64\n",
      "Unique labels in training: tensor([0, 1])\n",
      "\n",
      "Test data:\n",
      "x shape: torch.Size([1, 1250]), x dtype: torch.float32\n",
      "y shape: torch.Size([]), y dtype: torch.int64\n",
      "Unique labels in test: tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "# データセットとDataLoaderの作成\n",
    "data_dir = '../../data/processed/BP_npy/PulseDB'\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = BPDataset(data_dir,train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = BPDataset(data_dir,train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# トレーニングデータの確認\n",
    "x, y = train_dataset[0]\n",
    "print(\"Training data:\")\n",
    "print(f\"x shape: {x.shape}, x dtype: {x.dtype}\")\n",
    "print(f\"y shape: {y.shape}, y dtype: {y.dtype}\")\n",
    "print(f\"Unique labels in training: {torch.unique(train_dataset.y)}\")\n",
    "\n",
    "# テストデータの確認\n",
    "x, y = test_dataset[0]\n",
    "print(\"\\nTest data:\")\n",
    "print(f\"x shape: {x.shape}, x dtype: {x.dtype}\")\n",
    "print(f\"y shape: {y.shape}, y dtype: {y.dtype}\")\n",
    "print(f\"Unique labels in test: {torch.unique(test_dataset.y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from resnet1d import ResNet1D\n",
    "from se_resnet1d import resnet34\n",
    "def train_model(config):\n",
    "    \n",
    "    # Initialize Weights & Biases (wandb)\n",
    "    wandb.init(project=\"regression-training\", config=config)\n",
    "    config = wandb.config\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    # train_dataset = train_dataset(data_root=r\"..\\data\\processed\\BP_npy\\250107_1152\\p00\")\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    train_loader = train_dataloader\n",
    "    # val_dataset = test_dataset(data_len=-1,data_root=r\"..\\data\\processed\\BP_npy\\250107_1152\\p00\")\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    val_loader = test_dataloader\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\",device)\n",
    "    labels = np.load(f'{data_dir}/train_sbp_2labels.npy')\n",
    "    class_weights = compute_class_weights(labels)\n",
    "    print(class_weights)\n",
    "    class_weights = class_weights.to(device)  # 重みをGPUに移動\n",
    "\n",
    "    # Model, Loss, and Optimizer\n",
    "    # model = ResNet1D(**config[\"network\"])\n",
    "    model = resnet34(num_classes=config[\"network\"][\"n_classes\"],in_channels=1)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # mae = nn.L1Loss()  # Mean Squared Error Loss for regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    earlystopping = EarlyStopping(f\"{config.output_path}/best.pth\",config.patience,verbose=True)\n",
    "    model.to(device)\n",
    "\n",
    "    wandb.watch(model, log_freq=config.log_interval)\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        # running_loss_mae = 0.0\n",
    "        # Training phase with progress bar\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{config.epochs} - Training\", leave=False)\n",
    "        for batch_idx, (x,y) in enumerate(train_loader_tqdm):\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            # print(gt.shape,cond.shape)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            # print(outputs.device,y.device)\n",
    "            loss = criterion(outputs, y)\n",
    "            # loss_mae = mae(outputs, gt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % config.log_interval == 0:\n",
    "                wandb.log({\"train_loss_step\": loss.item()})\n",
    "            # if batch_idx  == 0:\n",
    "                # wandb.log({\"train/loss\": log_img(gt,outputs)})\n",
    "            running_loss += loss.item()\n",
    "            # running_loss_mae += loss_mae.item()\n",
    "            train_loader_tqdm.set_postfix(loss=running_loss/(batch_idx+1))\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        # train_loss_mae = running_loss_mae / len(train_loader)\n",
    "\n",
    "        # Validation phase with progress bar\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_y_true = []\n",
    "        all_y_pred = []\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{config.epochs} - Validation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x,y) in enumerate(val_loader_tqdm):\n",
    "\n",
    "                x,y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "                # loss_mae = mae(outputs, gt)\n",
    "                if batch_idx % config.log_interval == 0:\n",
    "                    wandb.log({\"test_loss_step\": loss.item()})\n",
    "                # if batch_idx  == 0:\n",
    "                #     wandb.log({\"val/loss\": log_img(gt,outputs)})\n",
    "                val_loss += loss.item()\n",
    "                # val_loss_mae += loss_mae.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_y_true.extend(y.cpu().numpy())\n",
    "                all_y_pred.extend(predicted.cpu().numpy())\n",
    "        all_y_true = np.array(all_y_true)\n",
    "        all_y_pred = np.array(all_y_pred)\n",
    "        accuracy, class_accuracies, f1 = calculate_metrics(all_y_true, all_y_pred,classes=config[\"network\"][\"n_classes\"])\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        # WandBでのログ記録例\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": val_loss,\n",
    "            \"accuracy\": accuracy.item(),\n",
    "            \"f1_macro\": f1,\n",
    "            \"class_0_accuracy\": class_accuracies[0].item(),\n",
    "            \"class_1_accuracy\": class_accuracies[1].item(),\n",
    "            # \"class_2_accuracy\": class_accuracies[2].item(),\n",
    "            # \"class_3_accuracy\": class_accuracies[3].item(),\n",
    "            \"conf_mat\": wandb.plot.confusion_matrix(y_true=all_y_true, preds=all_y_pred, class_names=[\"0\",\"1\"]),\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "        # val_loss_mae /= len(val_loader)\n",
    "        earlystopping(val_loss,model)\n",
    "        if earlystopping.early_stop:\n",
    "            print(\"Early Stopping!\")\n",
    "            wandb.finish()\n",
    "            break\n",
    "        # Log metrics to wandb\n",
    "        # wandb.log({\n",
    "        #     \"epoch\": epoch + 1,\n",
    "        #     \"train/loss_epoch\": train_loss,\n",
    "        #     \"train/mae\": train_loss_mae,\n",
    "        #     \"val/loss_epoch\": val_loss,\n",
    "        #     \"val/mae\": val_loss_mae\n",
    "        # })\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{config.epochs}]\"\n",
    "              f\" Train Loss: {train_loss:.4f}\"\n",
    "              f\" Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from se_resnet1d import resnet18mini as resnet\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "def train_model_clf_cv(config,fold=5):\n",
    "    output_warning(config)\n",
    "    data_dir = '../../data/processed/BP_npy/PulseDB'\n",
    "    # Initialize Weights & Biases (wandb)\n",
    "    wandb.init(project=\"regression-training\", config=config)\n",
    "    config = wandb.config\n",
    "    \n",
    "    for f in range(fold):\n",
    "        \n",
    "        train_dataset = BPDataset(data_dir,cv=True,fold=f,train=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "        val_dataset = BPDataset(data_dir,cv=True,fold=f,train=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"device:\",device)\n",
    "        unique, counts = np.unique(train_dataset.y, return_counts=True)\n",
    "        for label, count in zip(unique, counts):\n",
    "            print(f\"ラベル {label}: {count}件\")\n",
    "        unique, counts = np.unique(val_dataset.y, return_counts=True)\n",
    "        for label, count in zip(unique, counts):\n",
    "            print(f\"ラベル {label}: {count}件\")\n",
    "        # Model, Loss, and Optimizer\n",
    "        # model = ResNet1D(**config[\"network\"])\n",
    "        model = resnet(num_classes=config[\"network\"][\"n_classes\"],in_channels=1)\n",
    "        # criterion = nn.L1Loss()\n",
    "        labels = np.load(f'{data_dir}/train_sbp_2labels.npy')\n",
    "        with open(f\"{data_dir}/cv_5fold_2labels.pkl\", \"rb\") as file:\n",
    "            cv_idx = pickle.load(file) \n",
    "            labels = labels[cv_idx[0][f]]\n",
    "        class_weights = compute_class_weights(labels)\n",
    "        print(class_weights)\n",
    "        class_weights = class_weights.to(device)  # 重みをGPUに移動\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        # mae = nn.L1Loss()  # Mean Squared Error Loss for regression\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "        total_step_size = len(train_loader) * config.min_epochs\n",
    "        warmup_steps = int(total_step_size * 0.1)  # 10% of total steps\n",
    "        lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_step_size)\n",
    "\n",
    "        earlystopping = EarlyStopping(f\"{config.output_path}/best_fold{f}.pth\",config.patience,verbose=True)\n",
    "        model.to(device)\n",
    "\n",
    "        wandb.watch(model, log_freq=config.log_interval)\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            # running_loss_mae = 0.0\n",
    "            # Training phase with progress bar\n",
    "            train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{config.epochs} - Training\", leave=False)\n",
    "            for batch_idx, (x,y) in enumerate(train_loader_tqdm):\n",
    "                x,y = x.to(device), y.to(device)\n",
    "                # print(gt.shape,cond.shape)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x)\n",
    "                # print(\"Input shape:\", x.shape,y.shape,outputs.shape)\n",
    "                # print(\"Check for NaN:\", torch.isnan(x).any(),torch.isnan(y).any(),torch.isnan(outputs).any())\n",
    "                # print(\"Check for Inf:\", torch.isinf(x).any(),torch.isinf(y).any(),torch.isinf(outputs).any())\n",
    "                # print(outputs.device,y.device)\n",
    "                loss = criterion(outputs, y) \n",
    "                # loss_mae = mae(outputs, gt)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                if batch_idx % config.log_interval == 0:\n",
    "                    wandb.log({\"train_loss_step\": loss.item(),\n",
    "                               \"lr\": lr_scheduler.get_last_lr()[0]})\n",
    "                # if batch_idx  == 0:\n",
    "                    # wandb.log({\"train/loss\": log_img(gt,outputs)})\n",
    "                running_loss += loss.item()\n",
    "                # running_loss_mae += loss_mae.item()\n",
    "                train_loader_tqdm.set_postfix(loss=running_loss/(batch_idx+1))\n",
    "\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            # train_loss_mae = running_loss_mae / len(train_loader)\n",
    "\n",
    "            # Validation phase with progress bar\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            all_y_true = []\n",
    "            all_y_pred = []\n",
    "            val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{config.epochs} - Validation\", leave=False)\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (x,y) in enumerate(val_loader_tqdm):\n",
    "\n",
    "                    x,y = x.to(device), y.to(device)\n",
    "                    outputs = model(x)\n",
    "                    # print(outputs.device,y.device)\n",
    "                    loss = criterion(outputs,y) \n",
    "                    # loss_mae = mae(outputs, gt)\n",
    "                    if batch_idx % config.log_interval == 0:\n",
    "                        wandb.log({\"test_loss_step\": loss.item()})\n",
    "                    # if batch_idx  == 0:\n",
    "                    #     wandb.log({\"val/loss\": log_img(gt,outputs)})\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    # val_loss_mae += loss_mae.item()\n",
    "                    all_y_true.extend(np.array(y.cpu().numpy()))\n",
    "                    all_y_pred.extend(np.array(predicted.cpu().numpy()))\n",
    "            all_y_true = np.array(all_y_true)\n",
    "            all_y_pred = np.array(all_y_pred)\n",
    "            # print(all_y_true.shape,all_y_pred.shape)\n",
    "            accuracy, class_accuracies, f1 = calculate_metrics(all_y_true, all_y_pred,classes=config[\"network\"][\"n_classes\"])\n",
    "            # error = (all_y_true - all_y_pred)\n",
    "            # mse = np.mean(error**2,axis=0)\n",
    "            # rmse = np.sqrt(mse)\n",
    "            # std = np.std(error,axis=0)\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            # WandBでのログ記録例\n",
    "            wandb.log({\n",
    "                \"train_loss\": train_loss,\n",
    "                \"test_loss\": val_loss,\n",
    "                \"accuracy\": accuracy.item(),\n",
    "                \"f1_macro\": f1,\n",
    "                \"conf_mat\": wandb.plot.confusion_matrix(y_true=all_y_true, preds=all_y_pred, class_names=[\"0\",\"1\"]),\n",
    "                \"epoch\": epoch + 1\n",
    "            })\n",
    "            # val_loss /= len(val_loader)\n",
    "            # val_loss_mae /= len(val_loader)\n",
    "            earlystopping(val_loss,model)\n",
    "            if earlystopping.early_stop:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "            # Log metrics to wandb\n",
    "            # wandb.log({\n",
    "            #     \"epoch\": epoch + 1,\n",
    "            #     \"train/loss_epoch\": train_loss,\n",
    "            #     \"train/mae\": train_loss_mae,\n",
    "            #     \"val/loss_epoch\": val_loss,\n",
    "            #     \"val/mae\": val_loss_mae\n",
    "            # })\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{config.epochs}]\"\n",
    "                f\" Train Loss: {train_loss:.4f}\"\n",
    "                f\" Val Loss: {val_loss:.4f}\")\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"network\":{\n",
    "        \"in_channels\":1,\n",
    "        \"base_filters\":64,\n",
    "        \"kernel_size\":3,\n",
    "        \"stride\":1,\n",
    "        \"groups\":1,\n",
    "        \"n_block\":5,\n",
    "        \"n_classes\":2,\n",
    "        \"dropout\":0.5,\n",
    "    },\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\":100,\n",
    "    \"min_epochs\":15,\n",
    "    \"log_interval\":100,\n",
    "    \"input_shape\":[1,1250],\n",
    "    \"output_path\": \"..\\\\outputs\\\\resnet\\\\0415_2class_cv_res18mini3\",\n",
    "    \"patience\":5    \n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m(config)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output path ..\\outputs\\resnet\\0415_2class_cv_res18mini2 does not exist. Proceeding...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\minowa\\BloodPressureEstimation\\notebooks\\BP_regression\\wandb\\run-20250415_094938-sp4yy9n2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bsa_mh/regression-training/runs/sp4yy9n2' target=\"_blank\">true-violet-163</a></strong> to <a href='https://wandb.ai/bsa_mh/regression-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bsa_mh/regression-training' target=\"_blank\">https://wandb.ai/bsa_mh/regression-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bsa_mh/regression-training/runs/sp4yy9n2' target=\"_blank\">https://wandb.ai/bsa_mh/regression-training/runs/sp4yy9n2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249952, 1, 1250) (249952,)\n",
      "torch.Size([249952, 1, 1250]) torch.Size([249952])\n",
      "torch.Size([62488, 1, 1250]) torch.Size([62488])\n",
      "device: cuda\n",
      "ラベル 0: 137792件\n",
      "ラベル 1: 112160件\n",
      "ラベル 0: 33988件\n",
      "ラベル 1: 28500件\n",
      "tensor([0.9070, 1.1143])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.48952 --> 0.48952).  Saving model ...\n",
      "Epoch [1/100] Train Loss: 0.6514 Val Loss: 0.4895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.48952 --> 0.45872).  Saving model ...\n",
      "Epoch [2/100] Train Loss: 0.4696 Val Loss: 0.4587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n",
      "the best of loss: 0.45872\n",
      "Epoch [3/100] Train Loss: 0.4296 Val Loss: 0.4596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n",
      "the best of loss: 0.45872\n",
      "Epoch [4/100] Train Loss: 0.4098 Val Loss: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.45872 --> 0.45536).  Saving model ...\n",
      "Epoch [5/100] Train Loss: 0.3966 Val Loss: 0.4554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n",
      "the best of loss: 0.45536\n",
      "Epoch [6/100] Train Loss: 0.3856 Val Loss: 0.4606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n",
      "the best of loss: 0.45536\n",
      "Epoch [7/100] Train Loss: 0.3779 Val Loss: 0.4631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 5\n",
      "the best of loss: 0.45536\n",
      "Epoch [8/100] Train Loss: 0.3710 Val Loss: 0.4623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 5\n",
      "the best of loss: 0.45536\n",
      "Epoch [9/100] Train Loss: 0.3667 Val Loss: 0.4643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 5\n",
      "the best of loss: 0.45536\n",
      "Early Stopping!\n",
      "(249952, 1, 1250) (249952,)\n",
      "torch.Size([249952, 1, 1250]) torch.Size([249952])\n",
      "torch.Size([62488, 1, 1250]) torch.Size([62488])\n",
      "device: cuda\n",
      "ラベル 0: 136341件\n",
      "ラベル 1: 113611件\n",
      "ラベル 0: 35439件\n",
      "ラベル 1: 27049件\n",
      "tensor([0.9166, 1.1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.52253 --> 0.52253).  Saving model ...\n",
      "Epoch [1/100] Train Loss: 0.6483 Val Loss: 0.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.52253 --> 0.45994).  Saving model ...\n",
      "Epoch [2/100] Train Loss: 0.4791 Val Loss: 0.4599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.45994 --> 0.45843).  Saving model ...\n",
      "Epoch [3/100] Train Loss: 0.4290 Val Loss: 0.4584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n",
      "the best of loss: 0.45843\n",
      "Epoch [4/100] Train Loss: 0.4080 Val Loss: 0.4644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n",
      "the best of loss: 0.45843\n",
      "Epoch [5/100] Train Loss: 0.3927 Val Loss: 0.4638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 5\n",
      "the best of loss: 0.45843\n",
      "Epoch [6/100] Train Loss: 0.3829 Val Loss: 0.4627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 5\n",
      "the best of loss: 0.45843\n",
      "Epoch [7/100] Train Loss: 0.3755 Val Loss: 0.4700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 5\n",
      "the best of loss: 0.45843\n",
      "Early Stopping!\n",
      "(249952, 1, 1250) (249952,)\n",
      "torch.Size([249952, 1, 1250]) torch.Size([249952])\n",
      "torch.Size([62488, 1, 1250]) torch.Size([62488])\n",
      "device: cuda\n",
      "ラベル 0: 137503件\n",
      "ラベル 1: 112449件\n",
      "ラベル 0: 34277件\n",
      "ラベル 1: 28211件\n",
      "tensor([0.9089, 1.1114])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.51105 --> 0.51105).  Saving model ...\n",
      "Epoch [1/100] Train Loss: 0.6730 Val Loss: 0.5110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.51105 --> 0.42897).  Saving model ...\n",
      "Epoch [2/100] Train Loss: 0.4898 Val Loss: 0.4290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.42897 --> 0.42050).  Saving model ...\n",
      "Epoch [3/100] Train Loss: 0.4425 Val Loss: 0.4205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n",
      "the best of loss: 0.42050\n",
      "Epoch [4/100] Train Loss: 0.4210 Val Loss: 0.4271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.42050 --> 0.41619).  Saving model ...\n",
      "Epoch [5/100] Train Loss: 0.4060 Val Loss: 0.4162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n",
      "the best of loss: 0.41619\n",
      "Epoch [6/100] Train Loss: 0.3948 Val Loss: 0.4197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n",
      "the best of loss: 0.41619\n",
      "Epoch [7/100] Train Loss: 0.3869 Val Loss: 0.4298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 5\n",
      "the best of loss: 0.41619\n",
      "Epoch [8/100] Train Loss: 0.3788 Val Loss: 0.4240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 5\n",
      "the best of loss: 0.41619\n",
      "Epoch [9/100] Train Loss: 0.3744 Val Loss: 0.4248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 5\n",
      "the best of loss: 0.41619\n",
      "Early Stopping!\n",
      "(249952, 1, 1250) (249952,)\n",
      "torch.Size([249952, 1, 1250]) torch.Size([249952])\n",
      "torch.Size([62488, 1, 1250]) torch.Size([62488])\n",
      "device: cuda\n",
      "ラベル 0: 137339件\n",
      "ラベル 1: 112613件\n",
      "ラベル 0: 34441件\n",
      "ラベル 1: 28047件\n",
      "tensor([0.9100, 1.1098])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.51331 --> 0.51331).  Saving model ...\n",
      "Epoch [1/100] Train Loss: 0.6739 Val Loss: 0.5133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.51331 --> 0.47612).  Saving model ...\n",
      "Epoch [2/100] Train Loss: 0.4878 Val Loss: 0.4761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.47612 --> 0.47593).  Saving model ...\n",
      "Epoch [3/100] Train Loss: 0.4367 Val Loss: 0.4759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.47593 --> 0.47177).  Saving model ...\n",
      "Epoch [4/100] Train Loss: 0.4161 Val Loss: 0.4718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.47177 --> 0.46662).  Saving model ...\n",
      "Epoch [5/100] Train Loss: 0.4023 Val Loss: 0.4666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n",
      "the best of loss: 0.46662\n",
      "Epoch [6/100] Train Loss: 0.3917 Val Loss: 0.4780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n",
      "the best of loss: 0.46662\n",
      "Epoch [7/100] Train Loss: 0.3847 Val Loss: 0.4832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 5\n",
      "the best of loss: 0.46662\n",
      "Epoch [8/100] Train Loss: 0.3786 Val Loss: 0.4776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 5\n",
      "the best of loss: 0.46662\n",
      "Epoch [9/100] Train Loss: 0.3736 Val Loss: 0.4847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 5\n",
      "the best of loss: 0.46662\n",
      "Early Stopping!\n",
      "(249952, 1, 1250) (249952,)\n",
      "torch.Size([249952, 1, 1250]) torch.Size([249952])\n",
      "torch.Size([62488, 1, 1250]) torch.Size([62488])\n",
      "device: cuda\n",
      "ラベル 0: 138145件\n",
      "ラベル 1: 111807件\n",
      "ラベル 0: 33635件\n",
      "ラベル 1: 28853件\n",
      "tensor([0.9047, 1.1178])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.54057 --> 0.54057).  Saving model ...\n",
      "Epoch [1/100] Train Loss: 0.7218 Val Loss: 0.5406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.54057 --> 0.50525).  Saving model ...\n",
      "Epoch [2/100] Train Loss: 0.4716 Val Loss: 0.5052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.50525 --> 0.49699).  Saving model ...\n",
      "Epoch [3/100] Train Loss: 0.4237 Val Loss: 0.4970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n",
      "the best of loss: 0.49699\n",
      "Epoch [4/100] Train Loss: 0.4040 Val Loss: 0.4985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n",
      "the best of loss: 0.49699\n",
      "Epoch [5/100] Train Loss: 0.3912 Val Loss: 0.5033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 5\n",
      "the best of loss: 0.49699\n",
      "Epoch [6/100] Train Loss: 0.3818 Val Loss: 0.4976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 5\n",
      "the best of loss: 0.49699\n",
      "Epoch [7/100] Train Loss: 0.3738 Val Loss: 0.4988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.49699 --> 0.49390).  Saving model ...\n",
      "Epoch [8/100] Train Loss: 0.3684 Val Loss: 0.4939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n",
      "the best of loss: 0.49390\n",
      "Epoch [9/100] Train Loss: 0.3625 Val Loss: 0.5071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n",
      "the best of loss: 0.49390\n",
      "Epoch [10/100] Train Loss: 0.3592 Val Loss: 0.5070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 5\n",
      "the best of loss: 0.49390\n",
      "Epoch [11/100] Train Loss: 0.3573 Val Loss: 0.5042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 5\n",
      "the best of loss: 0.49390\n",
      "Epoch [12/100] Train Loss: 0.3546 Val Loss: 0.5060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 5\n",
      "the best of loss: 0.49390\n",
      "Early Stopping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93c3640b6814e8fa8a866bf0d6bcc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.037 MB of 0.037 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▄▆▆▇▆▇▆▆▂▆▆▆▇▆▆▇█▇████▂▅▅▆▆▆▅▆▁▄▄▄▅▅▅▅▅▅</td></tr><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▁▂▂▃▄▅▅▂▂▃▄▅▅▆▁▂▂▃▄▅▅▆▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>f1_macro</td><td>▄▆▆▆▆▇▇▆▂▆▆▆▆▆▆▇█▇████▂▅▅▆▆▆▅▆▁▄▄▄▅▅▅▅▅▅</td></tr><tr><td>lr</td><td>▂██▇▇▆▄▃▂██▇▆▅▂██▇▆▆▄▃▄██▇▆▅▄▃▄██▇▆▅▄▃▂▁</td></tr><tr><td>test_loss</td><td>▅▃▃▃▃▄▃▄▇▃▃▄▃▄▄▂▁▁▁▂▁▁▆▄▄▄▄▅▄▄█▆▆▆▆▆▆▆▆▆</td></tr><tr><td>test_loss_step</td><td>▂▂▂▁▃▂▁▃▂▁▂▂█▂▃▁▃▁▁▃▁▁▂▂▂▂▂▂▂▁▂▂▃▁▂▃▁▂▁▁</td></tr><tr><td>train_loss</td><td>▇▃▂▂▂▁▁▁▇▃▂▂▂▁▁▄▃▂▂▂▁▁▇▄▃▂▂▂▁▁█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▄▃▄▄▁█▃▃▅▄▂▇▂▃▃▃▃▂▂▅▄▄▃▄▃▂▂▇▆▄▄▃▃▁▄▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.7706</td></tr><tr><td>epoch</td><td>13</td></tr><tr><td>f1_macro</td><td>0.76941</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>test_loss</td><td>0.50259</td></tr><tr><td>test_loss_step</td><td>0.29179</td></tr><tr><td>train_loss</td><td>0.35373</td></tr><tr><td>train_loss_step</td><td>0.32837</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-violet-163</strong> at: <a href='https://wandb.ai/bsa_mh/regression-training/runs/sp4yy9n2' target=\"_blank\">https://wandb.ai/bsa_mh/regression-training/runs/sp4yy9n2</a><br/>Synced 5 W&B file(s), 51 media file(s), 51 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250415_094938-sp4yy9n2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model_clf_cv(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n        torchao not installed.\n        Please follow the instructions at https://pytorch.org/torchtune/main/install.html#pre-requisites\n        to install torchao.\n        ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mf:\\minowa\\BloodPressureEstimation\\.venv\\lib\\site-packages\\torchtune\\__init__.py:16\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchao\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mf:\\minowa\\BloodPressureEstimation\\.venv\\lib\\site-packages\\torchao\\__init__.py:41\u001b[0m\n\u001b[0;32m     39\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping import of cpp extensions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     autoquant,\n\u001b[0;32m     43\u001b[0m     quantize_,\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes, optim, testing\n",
      "File \u001b[1;32mf:\\minowa\\BloodPressureEstimation\\.venv\\lib\\site-packages\\torchao\\quantization\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     int_scaled_matmul,\n\u001b[0;32m      3\u001b[0m     safe_int_mm,\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautoquant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     ALL_AUTOQUANT_CLASS_LIST,\n\u001b[0;32m      8\u001b[0m     DEFAULT_AUTOQUANT_CLASS_LIST,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     autoquant,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mf:\\minowa\\BloodPressureEstimation\\.venv\\lib\\site-packages\\torchao\\kernel\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbsr_triton_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bsr_dense_addmm\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintmm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m int_scaled_matmul, safe_int_mm\n",
      "File \u001b[1;32mf:\\minowa\\BloodPressureEstimation\\.venv\\lib\\site-packages\\torchao\\kernel\\bsr_triton_ops.py:28\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_triton_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     broadcast_batch_dims,\n\u001b[0;32m     23\u001b[0m     launch_kernel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     tile_to_blocksize,\n\u001b[0;32m     27\u001b[0m )\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_triton_ops_meta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_meta, minimize, update\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_triton\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_triton\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch.sparse._triton_ops_meta'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cosine_schedule_with_warmup\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model_regr_cv\u001b[39m(model,config,fold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      3\u001b[0m     output_warning(config)\n",
      "File \u001b[1;32mf:\\minowa\\BloodPressureEstimation\\.venv\\lib\\site-packages\\torchtune\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchao\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m        torchao not installed.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m        Please follow the instructions at https://pytorch.org/torchtune/main/install.html#pre-requisites\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m        to install torchao.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Enables faster downloading. For more info: https://huggingface.co/docs/huggingface_hub/en/guides/download#faster-downloads\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# To disable, run `HF_HUB_ENABLE_HF_TRANSFER=0 tune download <model_config>`\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \n        torchao not installed.\n        Please follow the instructions at https://pytorch.org/torchtune/main/install.html#pre-requisites\n        to install torchao.\n        "
     ]
    }
   ],
   "source": [
    "from torchtune.modules import get_cosine_schedule_with_warmup\n",
    "def train_model_regr_cv(model,config,fold=5):\n",
    "    output_warning(config)\n",
    "    data_dir = '../../data/processed/BP_npy/PulseDB'\n",
    "    cv_idx_path = r\"../../data/processed/BP_npy/PulseDB/cv_5fold.pkl\"\n",
    "    with open(cv_idx_path, \"rb\") as f:\n",
    "        cv_idx = pickle.load(f)\n",
    "    # Initialize Weights & Biases (wandb)\n",
    "    wandb.init(project=\"regression-training\", config=config)\n",
    "    config = wandb.config\n",
    "    \n",
    "    for f in range(fold):\n",
    "        \n",
    "        train_dataset = BPDataset_Regr(data_dir,cv=cv_idx[0][f],train=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        val_dataset = BPDataset_Regr(data_dir,cv=cv_idx[1][f],train=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"device:\",device)\n",
    "\n",
    "        # Model, Loss, and Optimizer\n",
    "        criterion = nn.L1Loss()\n",
    "        # criterion = nn.CrossEntropyLoss()\n",
    "        # mae = nn.L1Loss()  # Mean Squared Error Loss for regression\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "        total_step_size = len(train_loader) * config.epochs\n",
    "        warmup_steps = int(total_step_size * 0.1)  # 10% of total steps\n",
    "        lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_step_size)\n",
    "        earlystopping = EarlyStopping(f\"{config.output_path}/best_fold{f}.pth\",config.patience,verbose=True)\n",
    "        model.to(device)\n",
    "\n",
    "        wandb.watch(model, log_freq=config.log_interval)\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            # running_loss_mae = 0.0\n",
    "            # Training phase with progress bar\n",
    "            train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{config.epochs} - Training\", leave=False)\n",
    "            for batch_idx, (x,sbp,dbp) in enumerate(train_loader_tqdm):\n",
    "                x,sbp,dbp = x.to(device), sbp.to(device),dbp.to(device)\n",
    "                # print(gt.shape,cond.shape)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x)\n",
    "                # print(outputs.device,y.device)\n",
    "                loss_sbp = criterion(outputs[:,0], sbp) \n",
    "                loss_dbp = criterion(outputs[:,1], dbp)\n",
    "                loss = loss_sbp + loss_dbp\n",
    "                # loss_mae = mae(outputs, gt)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                if batch_idx % config.log_interval == 0:\n",
    "                    wandb.log({\"train_loss_step\": loss.item()})\n",
    "                # if batch_idx  == 0:\n",
    "                    # wandb.log({\"train/loss\": log_img(gt,outputs)})\n",
    "                running_loss += loss.item()\n",
    "                # running_loss_mae += loss_mae.item()\n",
    "                train_loader_tqdm.set_postfix(loss=running_loss/(batch_idx+1))\n",
    "\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            # train_loss_mae = running_loss_mae / len(train_loader)\n",
    "\n",
    "            # Validation phase with progress bar\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            all_y_true = []\n",
    "            all_y_pred = []\n",
    "            val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{config.epochs} - Validation\", leave=False)\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (x,sbp,dbp) in enumerate(val_loader_tqdm):\n",
    "\n",
    "                    x,sbp,dbp = x.to(device), sbp.to(device),dbp.to(device)\n",
    "                    outputs = model(x)\n",
    "                    # print(outputs.device,y.device)\n",
    "                    loss_sbp = criterion(outputs[:,0], sbp) \n",
    "                    loss_dbp = criterion(outputs[:,1], dbp)\n",
    "                    loss = loss_sbp + loss_dbp\n",
    "                    # loss_mae = mae(outputs, gt)\n",
    "                    if batch_idx % config.log_interval == 0:\n",
    "                        wandb.log({\"test_loss_step\": loss.item()})\n",
    "                    # if batch_idx  == 0:\n",
    "                    #     wandb.log({\"val/loss\": log_img(gt,outputs)})\n",
    "                    val_loss += loss.item()\n",
    "                    # val_loss_mae += loss_mae.item()\n",
    "                    all_y_true.extend(np.array([sbp.cpu().numpy(),dbp.cpu().numpy()]))\n",
    "                    all_y_pred.extend(np.array([outputs[:,0].cpu().numpy(),outputs[:,1].cpu().numpy()]))\n",
    "            all_y_true = np.concatenate(all_y_true,axis=0).reshape(-1,2)\n",
    "            all_y_pred = np.concatenate(all_y_pred,axis=0).reshape(-1,2)\n",
    "            # print(all_y_true.shape,all_y_pred.shape)\n",
    "            # accuracy, class_accuracies, f1 = calculate_metrics(all_y_true, all_y_pred,classes=config[\"network\"][\"n_classes\"])\n",
    "            error = (all_y_true - all_y_pred)\n",
    "            mse = np.mean(error**2,axis=0)\n",
    "            rmse = np.sqrt(mse)\n",
    "            std = np.std(error,axis=0)\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            # WandBでのログ記録例\n",
    "            wandb.log({\n",
    "                \"train_loss\": train_loss,\n",
    "                \"test_loss\": val_loss,\n",
    "                \"mse_sbp\": mse[0],\n",
    "                \"rmse_sbp\": rmse[0],\n",
    "                \"std_sbp\": std[0],\n",
    "                \"mse_dbp\": mse[1],\n",
    "                \"rmse_dbp\": rmse[1],\n",
    "                \"std_dbp\": std[1],\n",
    "                # \"conf_mat\": wandb.plot.confusion_matrix(y_true=all_y_true, preds=all_y_pred, class_names=[\"0\",\"1\",\"2\",\"3\"]),\n",
    "                \"epoch\": epoch + 1\n",
    "            })\n",
    "            # val_loss /= len(val_loader)\n",
    "            # val_loss_mae /= len(val_loader)\n",
    "            earlystopping(val_loss,model)\n",
    "            if earlystopping.early_stop:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "            # Log metrics to wandb\n",
    "            # wandb.log({\n",
    "            #     \"epoch\": epoch + 1,\n",
    "            #     \"train/loss_epoch\": train_loss,\n",
    "            #     \"train/mae\": train_loss_mae,\n",
    "            #     \"val/loss_epoch\": val_loss,\n",
    "            #     \"val/mae\": val_loss_mae\n",
    "            # })\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{config.epochs}]\"\n",
    "                f\" Train Loss: {train_loss:.4f}\"\n",
    "                f\" Val Loss: {val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_regr(config):\n",
    "    \n",
    "    # Initialize Weights & Biases (wandb)\n",
    "    wandb.init(project=\"regression-training\", config=config)\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Dataset and DataLoader\n",
    "    # train_dataset = train_dataset(data_root=r\"..\\data\\processed\\BP_npy\\250107_1152\\p00\")\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    train_loader = train_dataloader\n",
    "    # val_dataset = test_dataset(data_len=-1,data_root=r\"..\\data\\processed\\BP_npy\\250107_1152\\p00\")\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    val_loader = test_dataloader\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\",device)\n",
    "\n",
    "        \n",
    "    # Model, Loss, and Optimizer\n",
    "    model = ResNet1D(**config[\"network\"])\n",
    "    criterion = nn.L1Loss()\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # mae = nn.L1Loss()  # Mean Squared Error Loss for regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    earlystopping = EarlyStopping(f\"{config.output_path}/best.pth\",config.patience,verbose=True)\n",
    "    model.to(device)\n",
    "\n",
    "    wandb.watch(model, log_freq=config.log_interval)\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        # running_loss_mae = 0.0\n",
    "        # Training phase with progress bar\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{config.epochs} - Training\", leave=False)\n",
    "        for batch_idx, (x,sbp,dbp) in enumerate(train_loader_tqdm):\n",
    "            x,sbp,dbp = x.to(device), sbp.to(device),dbp.to(device)\n",
    "            # print(gt.shape,cond.shape)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            # print(outputs.device,y.device)\n",
    "            loss_sbp = criterion(outputs[:,0], sbp) \n",
    "            loss_dbp = criterion(outputs[:,1], dbp)\n",
    "            loss = loss_sbp + loss_dbp\n",
    "            # loss_mae = mae(outputs, gt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % config.log_interval == 0:\n",
    "                wandb.log({\"train_loss_step\": loss.item()})\n",
    "            # if batch_idx  == 0:\n",
    "                # wandb.log({\"train/loss\": log_img(gt,outputs)})\n",
    "            running_loss += loss.item()\n",
    "            # running_loss_mae += loss_mae.item()\n",
    "            train_loader_tqdm.set_postfix(loss=running_loss/(batch_idx+1))\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        # train_loss_mae = running_loss_mae / len(train_loader)\n",
    "\n",
    "        # Validation phase with progress bar\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_y_true = []\n",
    "        all_y_pred = []\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{config.epochs} - Validation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x,sbp,dbp) in enumerate(val_loader_tqdm):\n",
    "\n",
    "                x,sbp,dbp = x.to(device), sbp.to(device),dbp.to(device)\n",
    "                outputs = model(x)\n",
    "                # print(outputs.device,y.device)\n",
    "                loss_sbp = criterion(outputs[:,0], sbp) \n",
    "                loss_dbp = criterion(outputs[:,1], dbp)\n",
    "                loss = loss_sbp + loss_dbp\n",
    "                # loss_mae = mae(outputs, gt)\n",
    "                if batch_idx % config.log_interval == 0:\n",
    "                    wandb.log({\"test_loss_step\": loss.item()})\n",
    "                # if batch_idx  == 0:\n",
    "                #     wandb.log({\"val/loss\": log_img(gt,outputs)})\n",
    "                val_loss += loss.item()\n",
    "                # val_loss_mae += loss_mae.item()\n",
    "                all_y_true.extend(np.array([sbp.cpu().numpy(),dbp.cpu().numpy()]))\n",
    "                all_y_pred.extend(np.array([outputs[:,0].cpu().numpy(),outputs[:,1].cpu().numpy()]))\n",
    "        all_y_true = np.concatenate(all_y_true,axis=0).reshape(-1,2)\n",
    "        all_y_pred = np.concatenate(all_y_pred,axis=0).reshape(-1,2)\n",
    "        # print(all_y_true.shape,all_y_pred.shape)\n",
    "        # accuracy, class_accuracies, f1 = calculate_metrics(all_y_true, all_y_pred,classes=config[\"network\"][\"n_classes\"])\n",
    "        error = (all_y_true - all_y_pred)\n",
    "        mse = np.mean(error**2,axis=0)\n",
    "        rmse = np.sqrt(mse)\n",
    "        std = np.std(error,axis=0)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        # WandBでのログ記録例\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": val_loss,\n",
    "            \"mse_sbp\": mse[0],\n",
    "            \"rmse_sbp\": rmse[0],\n",
    "            \"std_sbp\": std[0],\n",
    "            \"mse_dbp\": mse[1],\n",
    "            \"rmse_dbp\": rmse[1],\n",
    "            \"std_dbp\": std[1],\n",
    "            # \"conf_mat\": wandb.plot.confusion_matrix(y_true=all_y_true, preds=all_y_pred, class_names=[\"0\",\"1\",\"2\",\"3\"]),\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "        # val_loss /= len(val_loader)\n",
    "        # val_loss_mae /= len(val_loader)\n",
    "        earlystopping(val_loss,model)\n",
    "        if earlystopping.early_stop:\n",
    "            print(\"Early Stopping!\")\n",
    "            break\n",
    "        # Log metrics to wandb\n",
    "        # wandb.log({\n",
    "        #     \"epoch\": epoch + 1,\n",
    "        #     \"train/loss_epoch\": train_loss,\n",
    "        #     \"train/mae\": train_loss_mae,\n",
    "        #     \"val/loss_epoch\": val_loss,\n",
    "        #     \"val/mae\": val_loss_mae\n",
    "        # })\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{config.epochs}]\"\n",
    "              f\" Train Loss: {train_loss:.4f}\"\n",
    "              f\" Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# train_dataset = BPDataset_Regr(data_dir,train=True)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# トレーニングデータの確認\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m x, y1,y2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, x dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# データセットとDataLoaderの作成\n",
    "data_dir = '../../data/processed/BP_npy/PulseDB'\n",
    "batch_size = 32\n",
    "\n",
    "# train_dataset = BPDataset_Regr(data_dir,train=True)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# test_dataset = BPDataset_Regr(data_dir,train=False)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# トレーニングデータの確認\n",
    "x, y1,y2 = train_dataset[0]\n",
    "print(\"Training data:\")\n",
    "print(f\"x shape: {x.shape}, x dtype: {x.dtype}\")\n",
    "print(f\"y shape: {y1.shape}, y dtype: {y1.dtype}\")\n",
    "# print(f\"Unique labels in training: {torch.unique(train_dataset.y)}\")\n",
    "\n",
    "# テストデータの確認\n",
    "x, y1,y2 = test_dataset[0]\n",
    "print(\"\\nTest data:\")\n",
    "print(f\"x shape: {x.shape}, x dtype: {x.dtype}\")\n",
    "print(f\"y shape: {y1.shape}, y dtype: {y1.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4681), tensor(0.3178))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,i,u=train_dataset[0]\n",
    "i,u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: 24amj35 (bsa_mh). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"network\":{\n",
    "        \"in_channels\":1,\n",
    "        \"base_filters\":64,\n",
    "        \"kernel_size\":3,\n",
    "        \"stride\":1,\n",
    "        \"groups\":1,\n",
    "        \"n_block\":5,\n",
    "        \"n_classes\":2,\n",
    "    },\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\":100,\n",
    "    \"log_interval\":100,\n",
    "    \"input_shape\":[1,1250],\n",
    "    \"output_path\": \"..\\\\outputs\\\\resnet\\\\0308_regr\",\n",
    "    \"patience\":10    \n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "cv_idx_path = r\"../../data/processed/BP_npy/PulseDB/cv_5fold.pkl\"\n",
    "with open(cv_idx_path, \"rb\") as f:\n",
    "    cv_idx = pickle.load(f)\n",
    "print(len(cv_idx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet1D(**config[\"network\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output path ..\\outputs\\resnet\\0310_trans does not exist. Proceeding...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hxnts44a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94724431df904639ac08b2cacb112520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-leaf-88</strong> at: <a href='https://wandb.ai/bsa_mh/regression-training/runs/hxnts44a' target=\"_blank\">https://wandb.ai/bsa_mh/regression-training/runs/hxnts44a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250310_184228-hxnts44a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hxnts44a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610ed19daf564992a4ae7a6f969ae0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777932999, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\minowa\\BloodPressureEstimation\\notebooks\\BP_regression\\wandb\\run-20250310_184317-ucnkdfft</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bsa_mh/regression-training/runs/ucnkdfft' target=\"_blank\">jumping-brook-89</a></strong> to <a href='https://wandb.ai/bsa_mh/regression-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bsa_mh/regression-training' target=\"_blank\">https://wandb.ai/bsa_mh/regression-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bsa_mh/regression-training/runs/ucnkdfft' target=\"_blank\">https://wandb.ai/bsa_mh/regression-training/runs/ucnkdfft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model_regr_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m, in \u001b[0;36mtrain_model_regr_cv\u001b[1;34m(model, config, fold)\u001b[0m\n\u001b[0;32m      9\u001b[0m config \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(fold):\n\u001b[1;32m---> 13\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mBPDataset_Regr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m     val_dataset \u001b[38;5;241m=\u001b[39m BPDataset_Regr(data_dir,cv\u001b[38;5;241m=\u001b[39mcv_idx[\u001b[38;5;241m1\u001b[39m][f],train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m, in \u001b[0;36mBPDataset_Regr.__init__\u001b[1;34m(self, data_dir, cv, train)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_dir,cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m----> 5\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/train.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1250\u001b[39m)  \u001b[38;5;66;03m# Shape: (-1, 1250)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msbp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train_sbp.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Shape: (-1,)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train_dbp.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Shape: (-1,)\u001b[39;00m\n",
      "File \u001b[1;32mf:\\minowa\\BloodPressureEstimation\\.venv\\lib\\site-packages\\numpy\\lib\\npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[0;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32mf:\\minowa\\BloodPressureEstimation\\.venv\\lib\\site-packages\\numpy\\lib\\format.py:809\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[0;32m    822\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model_regr_cv(model,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# best_ckpt = \".\\\\\"+config[\"output_path\"]+\"\\\\best.pth\"\n",
    "# model = ResNet1D(config, \"not\").to(device)\n",
    "# model.load_state_dict(torch.load(best_ckpt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet1d import ResNet1D\n",
    "config = {\n",
    "    \"network\":{\n",
    "        \"in_channels\":1,\n",
    "        \"base_filters\":64,\n",
    "        \"kernel_size\":3,\n",
    "        \"stride\":1,\n",
    "        \"groups\":1,\n",
    "        \"n_block\":5,\n",
    "        \"n_classes\":2,\n",
    "    },\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\":100,\n",
    "    \"log_interval\":100,\n",
    "    \"input_shape\":[1,1250],\n",
    "    \"output_path\": \"..\\\\outputs\\\\resnet\\\\0310_trans\",\n",
    "    \"patience\":10    \n",
    "          }\n",
    "model = ResNet1D(**config[\"network\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('..\\\\outputs\\\\resnet\\\\0228\\\\best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "freeze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers_to_freeze = [model.first_block_conv,\n",
    "                    model.first_block_bn,\n",
    "                    model.first_block_relu,\n",
    "                    model.basicblock_list[0],\n",
    "                    model.basicblock_list[1]\n",
    "                    ]\n",
    "for layer in layers_to_freeze:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output path ..\\outputs\\resnet\\0310_trans does not exist. Proceeding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: 24amj35 (bsa_mh). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\minowa\\BloodPressureEstimation\\notebooks\\BP_regression\\wandb\\run-20250310_184228-hxnts44a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bsa_mh/regression-training/runs/hxnts44a' target=\"_blank\">polar-leaf-88</a></strong> to <a href='https://wandb.ai/bsa_mh/regression-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bsa_mh/regression-training' target=\"_blank\">https://wandb.ai/bsa_mh/regression-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bsa_mh/regression-training/runs/hxnts44a' target=\"_blank\">https://wandb.ai/bsa_mh/regression-training/runs/hxnts44a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model_regr_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m, in \u001b[0;36mtrain_model_regr_cv\u001b[1;34m(model, config, fold)\u001b[0m\n\u001b[0;32m      8\u001b[0m config \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(fold):\n\u001b[1;32m---> 12\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m BPDataset_Regr(\u001b[43mdata_dir\u001b[49m,cv\u001b[38;5;241m=\u001b[39mcv_idx[\u001b[38;5;241m0\u001b[39m][f],train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m     val_dataset \u001b[38;5;241m=\u001b[39m BPDataset_Regr(data_dir,cv\u001b[38;5;241m=\u001b[39mcv_idx[\u001b[38;5;241m1\u001b[39m][f],train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "train_model_regr_cv(model,config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
