{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "from torch import tensor as Tensor\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "import wandb\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import time\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://github.com/AntixK/PyTorch-VAE/blob/master/models/vanilla_vae.py\n",
    "# Tensor = TypeVar('torch.tensor')\n",
    "\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        # print()\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*8*8, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*8*8, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] *8*8)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 1,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result) #  nn.Linear\n",
    "        log_var = self.fc_var(result) # nn.Linear\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        # print(result.shape)\n",
    "        result = result.view(-1, 512, 8,8)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2\n",
    "conv2d mid layer  \n",
    "add groupnorm 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(in_channels, num_groups=32):\n",
    "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.k = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.v = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1,\n",
    "                                        padding=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b,c,h,w = q.shape\n",
    "        q = q.reshape(b,c,h*w)\n",
    "        q = q.permute(0,2,1)   # b,hw,c\n",
    "        k = k.reshape(b,c,h*w) # b,c,hw\n",
    "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = v.reshape(b,c,h*w)\n",
    "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
    "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
    "        h_ = h_.reshape(b,c,h,w)\n",
    "\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x+h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VanillaVAE_v2(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE_v2, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    Normalize(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.mid_params = nn.Conv2d(hidden_dims[-1],latent_dim*2,1)\n",
    "        self.decoder_input2 = nn.Conv2d(latent_dim, hidden_dims[-1],1)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "            # Build Decoder\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    Normalize(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            Normalize(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 1,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = self.mid_params(result)\n",
    "        mu,log_var = torch.chunk(result,2,dim=1)\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input2(z)\n",
    "\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        # print(\"input,decode(z),mu,var,z\",input.shape,self.decode(z).shape,mu.shape,log_var.shape,z.shape)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "        # kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(-torch.pow(mu, 2)\n",
    "                                       - log_var.exp() + 1.0 + log_var,\n",
    "                                       dim=[1, 2, 3]),dim=0)\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        # print(log_var.shape,mu.shape,torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1).shape,kld_loss.shape)\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAE(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                #  latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super(ConvAE, self).__init__()\n",
    "\n",
    "        # self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 1, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.MaxPool2d(2,2)),\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        # print()\n",
    "        # self.fc_mu = nn.Linear(hidden_dims[-1]*8*8, latent_dim)\n",
    "        # self.fc_var = nn.Linear(hidden_dims[-1]*8*8, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        # self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] *8*8)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 1,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        # print(input.shape)\n",
    "        result = self.encoder(input)\n",
    "        # print(result.shape)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        result = self.decoder(z)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        return [self.decode(self.encode(input)),input]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = torch.unsqueeze(args[1][:,0],1)\n",
    "        loss =F.mse_loss(recons,input)\n",
    "        # print(args[0].shape,args[1].shape,input.shape)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def make_dataset(dir):\n",
    "    print(os.path.isfile(dir))\n",
    "    if os.path.isfile(dir):\n",
    "        arr = np.genfromtxt(dir, dtype=str, encoding='utf-8')\n",
    "        if arr.ndim:\n",
    "            images = [i for i in arr]\n",
    "        else:\n",
    "            images = np.array([arr])\n",
    "    else:\n",
    "        images = []\n",
    "        assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "        for root, _, fnames in sorted(os.walk(dir)):\n",
    "            for fname in sorted(fnames):\n",
    "                if is_image_file(fname):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    images.append(path)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "class PPG2ABPDataset_v3_base(Dataset):\n",
    "    def __init__(self,data_flist,data_root = r\"F:\\minowa\\BloodPressureEstimation\\data\\processed\\BP_npy\\0325_256_corr_clean\\p00\",\n",
    "                 data_len=1000, size=224, loader=None):\n",
    "        self.data_root = data_root\n",
    "        self.data_flist = data_flist\n",
    "        self.flist = make_dataset(self.data_flist)\n",
    "        # if data_len > 0:\n",
    "        #     self.flist = flist[:int(data_len)]\n",
    "        # else:\n",
    "        #     self.flist = flist\n",
    "        self.tfs = transforms.ToTensor()\n",
    "        self.size = size\n",
    "        self.data=self.load_npys()\n",
    "        if data_len > 0:\n",
    "            data_index = np.arange(0,len(self.data),max(len(self.data)//int(data_len),0)).astype(int)[:int(data_len)]\n",
    "            self.data = self.data[data_index]\n",
    "        else:\n",
    "            self.data = self.data[:len(self.data)-len(self.data)%64]\n",
    "        print(\"data prepared:\" ,self.data.shape)\n",
    "    # def _expand_dims(self,tensor):\n",
    "    #     length = tensor.shape[-1]\n",
    "    #     reshaped = torch.unsqueeze(tensor, axis=2)\n",
    "    #     reshaped = torch.repeat_interleave(reshaped, length, axis=2)\n",
    "    #     return reshaped\n",
    "    def load_npys(self):\n",
    "        data = []\n",
    "        for f in self.flist:\n",
    "            arr = np.load(self.data_root+\"\\\\\"+str(f))\n",
    "            if len(arr) != 0:\n",
    "                data.append(arr)\n",
    "        data = np.concatenate(data)\n",
    "        return data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # ret = {}\n",
    "        # ret['gt_image'] = self._expand_dims(torch.from_numpy(self.data[index,:,0].astype(np.float32)))\n",
    "        # ret['cond_image'] = self._expand_dims(torch.from_numpy(self.data[index,:,1].astype(np.float32)))\n",
    "        ret = self.data[index,:,1].astype(np.float32)\n",
    "        ret = np.tile(ret,(256,1))[np.newaxis]\n",
    "        # ret['path'] = str(index)\n",
    "        return ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "class PPG2ABPDataset_v3_Train(PPG2ABPDataset_v3_base):\n",
    "    def __init__(self, data_len=-1, size=224, loader=None):\n",
    "        super().__init__(data_flist = r\"F:\\minowa\\BloodPressureEstimation\\data\\processed\\list\\train_BP2.txt\",data_len=data_len,size=size)\n",
    "\n",
    "class PPG2ABPDataset_v3_Val(PPG2ABPDataset_v3_base):\n",
    "    def __init__(self, data_len=-1, size=224, loader=None):\n",
    "        super().__init__(data_flist = r\"F:\\minowa\\BloodPressureEstimation\\data\\processed\\list\\val_BP2.txt\",data_len=data_len,size=size)\n",
    "\n",
    "class PPG2ABPDataset_v3_Test(PPG2ABPDataset_v3_base):\n",
    "    def __init__(self, data_len=-1, size=224, loader=None):\n",
    "        super().__init__(data_flist = r\"F:\\minowa\\BloodPressureEstimation\\data\\processed\\list\\test_BP2.txt\",data_len=data_len,size=size)         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v4\n",
    "multi channel input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPG2ABPDataset_v4_base(Dataset):\n",
    "    def __init__(self,data_flist,data_root = r\"..\\data\\processed\\BP_npy\\0625_256_2_diff_clean\\p00\",\n",
    "                 data_len=1000, size=224, loader=None, use_ppg = False):\n",
    "        self.data_root = data_root\n",
    "        self.data_flist = data_flist\n",
    "        self.flist = make_dataset(self.data_flist)\n",
    "        # if data_len > 0:\n",
    "        #     self.flist = flist[:int(data_len)]\n",
    "        # else:\n",
    "        #     self.flist = flist\n",
    "        self.use_ppg = int(use_ppg)\n",
    "        self.tfs = transforms.ToTensor()\n",
    "        self.size = size\n",
    "        self.data=self.load_npys()\n",
    "        if data_len > 0:\n",
    "            data_index = np.arange(0,len(self.data),max(len(self.data)//int(data_len),0)).astype(int)[:int(data_len)]\n",
    "            self.data = self.data[data_index]\n",
    "        else:\n",
    "            self.data = self.data[:len(self.data)-len(self.data)%64]\n",
    "        print(\"data prepared:\" ,self.data.shape)\n",
    "    def load_npys(self):\n",
    "        data = []\n",
    "        i = self.use_ppg\n",
    "        for f in self.flist:\n",
    "            arr = np.load(self.data_root+\"\\\\\"+str(f))[:,:,i]\n",
    "            arr = arr.squeeze().transpose(0,2,1) # (-1, 256,  3) -> (-1, 3, 256)\n",
    "            if len(arr) != 0:\n",
    "                data.append(arr)\n",
    "        data = np.concatenate(data)\n",
    "        return data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ret = self.data[index].astype(np.float32) # (3,256)\n",
    "        ret = np.tile(ret[:, np.newaxis, :], (1, 256, 1)) # (-1, 3, 256, 256)\n",
    "        return ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "class PPG2ABPDataset_v4_Train(PPG2ABPDataset_v4_base):\n",
    "    def __init__(self, data_len=-1, size=224, loader=None,use_ppg=False):\n",
    "        super().__init__(data_flist = r\"..\\data\\processed\\list\\train_BP2.txt\",data_len=data_len,size=size,use_ppg=use_ppg)\n",
    "\n",
    "class PPG2ABPDataset_v4_Val(PPG2ABPDataset_v4_base):\n",
    "    def __init__(self, data_len=-1, size=224, loader=None,use_ppg=False):\n",
    "        super().__init__(data_flist = r\"..\\data\\processed\\list\\val_BP2.txt\",data_len=data_len,size=size,use_ppg=use_ppg)\n",
    "\n",
    "class PPG2ABPDataset_v4_Test(PPG2ABPDataset_v4_base):\n",
    "    def __init__(self, data_len=-1, size=224, loader=None,use_ppg=False):\n",
    "        super().__init__(data_flist = r\"..\\data\\processed\\list\\test_BP2.txt\",data_len=data_len,size=size,use_ppg=use_ppg)         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model,epoch):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model,epoch)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model,epoch)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model,epoch):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path.format(t=epoch))\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader,optimizer, model,kld_weight=0.00025):\n",
    "    # optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss_sum=0\n",
    "    recon_loss_sum=0\n",
    "    kld_loss_sum = 0\n",
    "    total=0\n",
    "    with tqdm(total=len(dataloader),unit=\"batch\") as pbar:\n",
    "        # with torch.autograd.set_detect_anomaly(True):\n",
    "        for batch, X in enumerate(dataloader):\n",
    "            X = X.to(device)\n",
    "            \n",
    "            # print(X.shape)\n",
    "            # print(X.shape,y.shape)\n",
    "            # Compute prediction error\n",
    "            results = model.forward(X)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss_function(*results, M_N=kld_weight)\n",
    "            \n",
    "            # Backpropagation\n",
    "            total+=X.size(0)\n",
    "            # {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "            total_loss_sum+=loss['loss']*X.size(0) \n",
    "            # recon_loss_sum+=loss['Reconstruction_Loss']*X.size(0) \n",
    "            # kld_loss_sum+=loss['KLD']*X.size(0)\n",
    "            running_total_loss=total_loss_sum/total\n",
    "            # running_recon_loss=recon_loss_sum/total\n",
    "            # running_kld_loss=kld_loss_sum/total\n",
    "            loss['loss'].backward()\n",
    "            # print(X.grad)\n",
    "            optimizer.step()\n",
    "            metrics = {\"train/total_loss_batch\":loss['loss'].item(),\n",
    "                    #    \"train/recon_loss_batch\":loss['Reconstruction_Loss'].item(),\n",
    "                    #    \"train/KLD_batch\":loss['KLD'].item(),\n",
    "                       \"train/total_loss\":running_total_loss.item(),\n",
    "                    #    \"train/recon_loss(MSE)\":running_recon_loss.item(),\n",
    "                    #    \"train/KLD\":running_kld_loss.item(),\n",
    "                    #    \"Total\":total\n",
    "                       }\n",
    "            if batch < 100:\n",
    "                # ðŸ Log train metrics to wandb \n",
    "                wandb.log(metrics)\n",
    "            # if batch < 50:\n",
    "                # wandb.log(metrics)\n",
    "                # torch.save(model.state_dict(),\"./wandb/weights.pth\")\n",
    "                # artifact = wandb.Artifact('model-weights',type='weights')\n",
    "                # artifact.add_file(\"./wandb/weights.pth\")\n",
    "                # wandb.log_artifact(artifact)\n",
    "                # time.sleep(10)\n",
    "            pbar.set_postfix(metrics)\n",
    "            pbar.update(1)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, log_images=2,kld_weight=0.00025,save = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    total=0\n",
    "    test_total_loss_sum=0\n",
    "    test_recon_loss_sum=0\n",
    "    test_kld_loss_sum = 0\n",
    "    images = []\n",
    "    gt = []\n",
    "    log_arr = np.random.randint(0,num_batches)\n",
    "    with torch.no_grad():\n",
    "        for i,X in enumerate(dataloader):\n",
    "            X = X.to(device)\n",
    "            total+=X.size(0)\n",
    "            # print(\"x\",X.shape)\n",
    "            results = model.forward(X)\n",
    "            loss = model.loss_function(*results, M_N=kld_weight)\n",
    "            if save:\n",
    "                images.append(model.generate(X).detach().cpu().numpy())\n",
    "                gt.append(X.detach().cpu().numpy())\n",
    "            test_total_loss_sum+=loss['loss']*X.size(0) \n",
    "            # test_recon_loss_sum+=loss['Reconstruction_Loss']*X.size(0) \n",
    "            # test_kld_loss_sum+=loss['KLD']*X.size(0)\n",
    "            if i == log_arr: \n",
    "                pred = model.generate(X)\n",
    "                print(pred.shape)\n",
    "                pred_image = wandb.Image(pred[0].detach().permute(1,2,0).cpu().numpy(),mode='L', caption=f\"generated {i*X.shape[0]}\")\n",
    "                gt_image = wandb.Image(X[0].detach().permute(1,2,0).cpu().numpy(),mode='RGB', caption=f\"ground truth {i*X.shape[0]}\")\n",
    "                wandb.log({\"val/pred_image\":pred_image,\"val/gt_image\":gt_image})\n",
    "    test_total_loss_sum /= total\n",
    "    # test_recon_loss_sum /= total\n",
    "    # test_kld_loss_sum /= total\n",
    "    wandb.log({\"val/total_loss\":test_total_loss_sum.item(),\n",
    "                    #    \"val/recon_loss(MSE)\":test_recon_loss_sum.item(),\n",
    "                    #    \"val/KLD\":test_kld_loss_sum.item(),\n",
    "                       })\n",
    "    # print(f\"Test Error: {test_total_loss_sum.item():>8f}\\nMSE: {test_recon_loss_sum.item():>8f} \\nKLD: {test_kld_loss_sum.item():>8f} \\n\")\n",
    "    if save:\n",
    "        images = np.concatenate(images,axis=0)\n",
    "        gt = np.concatenate(gt,axis=0)\n",
    "    return test_total_loss_sum.item(),[images,gt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m24amj35\u001b[0m (\u001b[33mbsa_mh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "data prepared: (30000, 3, 256)\n",
      "True\n",
      "data prepared: (1000, 3, 256)\n",
      "True\n",
      "data prepared: (2000, 3, 256)\n"
     ]
    }
   ],
   "source": [
    "# model = VanillaVAE_v2(in_channels = 1, latent_dim=4,hidden_dims=[64,128,256,512]).to(device)\n",
    "use_ppg = True\n",
    "model = ConvAE(in_channels = 3,hidden_dims=[64,128,256]).to(device)\n",
    "# model = VanillaVAE(in_channels = 1, latent_dim=32).to(device)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=PPG2ABPDataset_v4_Train(data_len=30000,use_ppg=use_ppg), \n",
    "                                         batch_size=16, \n",
    "                                         shuffle=True, )\n",
    "val_loader = torch.utils.data.DataLoader(dataset=PPG2ABPDataset_v4_Val(data_len=1000,use_ppg=use_ppg), \n",
    "                                         batch_size=16, \n",
    "                                         shuffle=False, )\n",
    "test_loader = torch.utils.data.DataLoader(dataset=PPG2ABPDataset_v4_Test(data_len=2000,use_ppg=use_ppg), \n",
    "                                          batch_size=16, \n",
    "                                         shuffle=False, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00048828125"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(\".\\outputs\\\\0625_vae_bp\\\\{t}.pth\")\n",
    "kld_weight = 32/(256**2) # \n",
    "epochs = 100\n",
    "lr = 0.0001\n",
    "kld_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\minowa\\BloodPressureEstimation\\notebooks\\wandb\\run-20240627_052106-wvfz5idq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bsa_mh/VAE/runs/wvfz5idq' target=\"_blank\">fiery-resonance-14</a></strong> to <a href='https://wandb.ai/bsa_mh/VAE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bsa_mh/VAE' target=\"_blank\">https://wandb.ai/bsa_mh/VAE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bsa_mh/VAE/runs/wvfz5idq' target=\"_blank\">https://wandb.ai/bsa_mh/VAE/runs/wvfz5idq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"VAE\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"VanillaVAE_v2\",\n",
    "    \"dataset\": \"bpdataset_bp\" if use_ppg else 'bpdataset_bp',\n",
    "    \"epochs\": epochs,\n",
    "    }\n",
    ")\n",
    "wandb.watch(model, log_freq=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00048828125\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ConvAE                                   [16, 1, 256, 256]         --\n",
      "â”œâ”€Sequential: 1-1                        [16, 256, 32, 32]         --\n",
      "â”‚    â””â”€Sequential: 2-1                   [16, 64, 128, 128]        --\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-1                  [16, 64, 256, 256]        1,792\n",
      "â”‚    â”‚    â””â”€BatchNorm2d: 3-2             [16, 64, 256, 256]        128\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-3               [16, 64, 256, 256]        --\n",
      "â”‚    â”‚    â””â”€MaxPool2d: 3-4               [16, 64, 128, 128]        --\n",
      "â”‚    â””â”€Sequential: 2-2                   [16, 128, 64, 64]         --\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-5                  [16, 128, 128, 128]       73,856\n",
      "â”‚    â”‚    â””â”€BatchNorm2d: 3-6             [16, 128, 128, 128]       256\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-7               [16, 128, 128, 128]       --\n",
      "â”‚    â”‚    â””â”€MaxPool2d: 3-8               [16, 128, 64, 64]         --\n",
      "â”‚    â””â”€Sequential: 2-3                   [16, 256, 32, 32]         --\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-9                  [16, 256, 64, 64]         295,168\n",
      "â”‚    â”‚    â””â”€BatchNorm2d: 3-10            [16, 256, 64, 64]         512\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-11              [16, 256, 64, 64]         --\n",
      "â”‚    â”‚    â””â”€MaxPool2d: 3-12              [16, 256, 32, 32]         --\n",
      "â”œâ”€Sequential: 1-2                        [16, 64, 128, 128]        --\n",
      "â”‚    â””â”€Sequential: 2-4                   [16, 128, 64, 64]         --\n",
      "â”‚    â”‚    â””â”€ConvTranspose2d: 3-13        [16, 128, 64, 64]         295,040\n",
      "â”‚    â”‚    â””â”€BatchNorm2d: 3-14            [16, 128, 64, 64]         256\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-15              [16, 128, 64, 64]         --\n",
      "â”‚    â””â”€Sequential: 2-5                   [16, 64, 128, 128]        --\n",
      "â”‚    â”‚    â””â”€ConvTranspose2d: 3-16        [16, 64, 128, 128]        73,792\n",
      "â”‚    â”‚    â””â”€BatchNorm2d: 3-17            [16, 64, 128, 128]        128\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-18              [16, 64, 128, 128]        --\n",
      "â”œâ”€Sequential: 1-3                        [16, 1, 256, 256]         --\n",
      "â”‚    â””â”€ConvTranspose2d: 2-6              [16, 64, 256, 256]        36,928\n",
      "â”‚    â””â”€BatchNorm2d: 2-7                  [16, 64, 256, 256]        128\n",
      "â”‚    â””â”€LeakyReLU: 2-8                    [16, 64, 256, 256]        --\n",
      "â”‚    â””â”€Conv2d: 2-9                       [16, 1, 256, 256]         577\n",
      "â”‚    â””â”€Tanh: 2-10                        [16, 1, 256, 256]         --\n",
      "==========================================================================================\n",
      "Total params: 778,561\n",
      "Trainable params: 778,561\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 118.59\n",
      "==========================================================================================\n",
      "Input size (MB): 12.58\n",
      "Forward/backward pass size (MB): 3363.83\n",
      "Params size (MB): 3.11\n",
      "Estimated Total Size (MB): 3379.53\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(kld_weight)\n",
    "model_summary = summary(model,input_size=[16,3,256,256])\n",
    "print(model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [03:40<00:00,  8.50batch/s, train/total_loss_batch=0.000145, train/total_loss=0.00116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256])\n",
      "Validation loss decreased (inf --> 0.000074).  Saving model ...\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [03:31<00:00,  8.87batch/s, train/total_loss_batch=1.46e-5, train/total_loss=2.27e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256])\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [03:31<00:00,  8.87batch/s, train/total_loss_batch=6.35e-6, train/total_loss=1.44e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256])\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [03:27<00:00,  9.03batch/s, train/total_loss_batch=1.26e-5, train/total_loss=1.12e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256])\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [03:27<00:00,  9.04batch/s, train/total_loss_batch=1.18e-5, train/total_loss=8.46e-6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256])\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 6\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [03:27<00:00,  9.04batch/s, train/total_loss_batch=2.51e-6, train/total_loss=7.16e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256])\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 7\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [03:27<00:00,  9.04batch/s, train/total_loss_batch=2.19e-5, train/total_loss=6.04e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256])\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 8\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [03:27<00:00,  9.06batch/s, train/total_loss_batch=2.57e-6, train/total_loss=5.12e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256])\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 9\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [03:25<00:00,  9.11batch/s, train/total_loss_batch=1.79e-6, train/total_loss=4.81e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256])\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [03:25<00:00,  9.11batch/s, train/total_loss_batch=2.2e-6, train/total_loss=3.96e-6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256])\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 11\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [03:25<00:00,  9.11batch/s, train/total_loss_batch=1.36e-6, train/total_loss=3.79e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256])\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "best epoch is epoch 0\n",
      "torch.Size([16, 1, 256, 256])\n",
      "(2000, 1, 256, 256) (2000, 3, 256, 256)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/total_loss</td><td>â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/total_loss_batch</td><td>â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val/total_loss</td><td>â–ˆâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/total_loss</td><td>0.0</td></tr><tr><td>train/total_loss_batch</td><td>0.0</td></tr><tr><td>val/total_loss</td><td>7e-05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-resonance-14</strong> at: <a href='https://wandb.ai/bsa_mh/VAE/runs/wvfz5idq' target=\"_blank\">https://wandb.ai/bsa_mh/VAE/runs/wvfz5idq</a><br/>Synced 5 W&B file(s), 24 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240627_052106-wvfz5idq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.95542839821428e-05"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True,path=path,delta=0.0002)\n",
    "os.makedirs(os.path.dirname(path),exist_ok=True)\n",
    "try:\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        # train(train_dataloader, model, loss_fn,loss_mae, optimizer)\n",
    "        train(train_loader, optimizer,model,kld_weight=kld_weight)\n",
    "        val_loss,_ = test(val_loader, model,kld_weight=kld_weight)\n",
    "        early_stopping(val_loss, model,t)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        # if t % 5 ==0: \n",
    "            # torch.save(model.state_dict(),path.format(t=t))\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model.state_dict(),path.format(t=\"latest\"))\n",
    "finally:\n",
    "    torch.save(model.state_dict(),path.format(t=\"latest\"))\n",
    "# load best epoch\n",
    "\n",
    "paths = glob.glob(os.path.join(os.path.dirname(path),\"[0-9]*.pth\"))\n",
    "best_epoch = sorted([int(os.path.splitext(os.path.basename(p))[0]) for p in paths])[-1]\n",
    "print('best epoch is epoch',best_epoch)\n",
    "model.load_state_dict(torch.load(path.format(t=best_epoch)))\n",
    "loss, images = test(test_loader,model,kld_weight=kld_weight,save=True)\n",
    "print(images[0].shape,images[1].shape)\n",
    "images[0] = images[0].squeeze()\n",
    "images[1] = images[1].squeeze()\n",
    "np.savez(os.path.join(os.path.dirname(path),'output.npz'),out=images[0],gt=images[1])\n",
    "wandb.finish()\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 256, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i  =iter(test_loader)\n",
    "x = i.__next__()\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "def im(arr):\n",
    " return Image.fromarray(((arr[0]+1)*255).detach().cpu().numpy().astype(np.uint8).squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.load(r\"F:\\minowa\\BloodPressureEstimation\\data\\processed\\BP_npy\\0325_256_corr_clean\\p00\\validate.npy\")[2,:,0]\n",
    "ret = np.tile(arr,(256,1))[np.newaxis].squeeze()\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAACiElEQVR4nO3QwY3bMAAFUYqi60n/xYnMwfZmO3iHzEAioAM/RnP9mfO+13qt11r3Pee8xhhnn733fvbz7P3ss/fZ55xx3sf7+fde15z3Wuv1WmutOed1jXHOPvvZz97P3vuzcM543x5njJ+PM8645q+Ne97XHONn5D3xnP3e+N75HF+TcV3zvtf9Wq/1+ZnxFdlvlV8in5U5/nMKoAU0BdACmgJoAU0BtICmAFpAUwAtoCmAFtAUQAtoCqAFNAXQApoCaAFNAbSApgBaQFMALaApgBbQFEALaAqgBTQF0AKaAmgBTQG0gKYAWkBTAC2gKYAW0BRAC2gKoAU0BdACmgJoAU0BtICmAFpAUwAtoCmAFtAUQAtoCqAFNAXQApoCaAFNAbSApgBaQFMALaApgBbQFEALaAqgBTQF0AKaAmgBTQG0gKYAWkBTAC2gKYAW0BRAC2gKoAU0BdACmgJoAU0BtICmAFpAUwAtoCmAFtAUQAtoCqAFNAXQApoCaAFNAbSApgBaQFMALaApgBbQFEALaAqgBTQF0AKaAmgBTQG0gKYAWkBTAC2gKYAW0BRAC2gKoAU0BdACmgJoAU0BtICmAFpAUwAtoCmAFtAUQAtoCqAFNAXQApoCaAFNAbSApgBaQFMALaApgBbQFEALaAqgBTQF0AKaAmgBTQG0gKYAWkBTAC2gKYAW0BRAC2gKoAU0BdACmgJoAU0BtICmAFpAUwAtoCmAFtAUQAtoCqAFNAXQApoCaAFNAbSApgBaQFMALaApgBbQFEALaAqgBTQF0AKaAmgBTQG0gKYAWkBTAC2gKYAW0BRAC2gKoAU0BdACmgJoAU0BtICmAFpAUwAtoCmAFtD8BakZkDfTjDTRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(((ret+1)*255).astype(np.uint8).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 256), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\minowa\\BloodPressureEstimation\\.venv\\lib\\site-packages\\PIL\\Image.py:3098\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3098\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   3099\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 256), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m, in \u001b[0;36mim\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mim\u001b[39m(arr):\n\u001b[1;32m----> 3\u001b[0m  \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\minowa\\BloodPressureEstimation\\.venv\\lib\\site-packages\\PIL\\Image.py:3102\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3100\u001b[0m         typekey_shape, typestr \u001b[38;5;241m=\u001b[39m typekey\n\u001b[0;32m   3101\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypekey_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypestr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 3102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   3103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3104\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 256), |u1"
     ]
    }
   ],
   "source": [
    "im(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAACrklEQVR4nO3QSZIaQRAFUWYaGljo/helqEy0QNMN3kLu67Afbr79sd0fT5fv2/3xeNzv1+vX+XQ47g+7/Xa7ec91XZbluTyX17quc8w5x5zjD+sYY4w5xpxjfnjP94fP7Xuz3R9Pl+vtcX887vfvy+V8Oh4P+91uu32/x3i9lufyXJZlXdfx2Rhz/PNijDE/8+8/8/Mz/9nfHY5fl9vt/njcb9/Xy/l8Ohz3+/1u81d/WV6/18ec49f8HGOMudv85xRAC2gKoAU0BdACmgJoAU0BtICmAFpAUwAtoCmAFtAUQAtoCqAFNAXQApoCaAFNAbSApgBaQFMALaApgBbQFEALaAqgBTQF0AKaAmgBTQG0gKYAWkBTAC2gKYAW0BRAC2gKoAU0BdACmgJoAU0BtICmAFpAUwAtoCmAFtAUQAtoCqAFNAXQApoCaAFNAbSApgBaQFMALaApgBbQFEALaAqgBTQF0AKaAmgBTQG0gKYAWkBTAC2gKYAW0BRAC2gKoAU0BdACmgJoAU0BtICmAFpAUwAtoCmAFtAUQAtoCqAFNAXQApoCaAFNAbSApgBaQFMALaApgBbQFEALaAqgBTQF0AKaAmgBTQG0gKYAWkBTAC2gKYAW0BRAC2gKoAU0BdACmgJoAU0BtICmAFpAUwAtoCmAFtAUQAtoCqAFNAXQApoCaAFNAbSApgBaQFMALaApgBbQFEALaAqgBTQF0AKaAmgBTQG0gKYAWkBTAC2gKYAW0BRAC2gKoAU0BdACmgJoAU0BtICmAFpAUwAtoCmAFtAUQAtoCqAFNAXQApoCaAFNAbSApgBaQFMALaApgBbQFEALaAqgBTQF0AKaAmgBTQG0gKYAWkBTAC2gKYAW0BRAC2gKoAU0BdACmgJoAU0BtICmAFpAUwAtoPkJJNSwUNHCuzUAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im(model.generate(x.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=256x256>,\n",
       " <PIL.Image.Image image mode=L size=256x256>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
