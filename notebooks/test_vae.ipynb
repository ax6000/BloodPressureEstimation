{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "__author__ = 'arbellea@post.bgu.ac.il'\n",
    "\n",
    "try:\n",
    "    import tensorflow.python.keras as k\n",
    "except AttributeError:\n",
    "    import tensorflow.keras as k\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "DEFAULT_NET_DOWN_PARAMS = {\n",
    "    'down_conv_kernels': [\n",
    "        [(5, 128), (5, 128)],\n",
    "        [(5, 256), (5, 256)],\n",
    "        [(5, 256), (5, 256)],\n",
    "        [(5, 512), (5, 512)],\n",
    "    ],\n",
    "    'lstm_kernels': [\n",
    "        [(5, 128)],\n",
    "        [(5, 256)],\n",
    "        [(5, 256)],\n",
    "        [(5, 512)],\n",
    "    ],\n",
    "    'up_conv_kernels': [\n",
    "        [(5, 256), (5, 256)],\n",
    "        [(5, 128), (5, 128)],\n",
    "        [(5, 64), (5, 64)],\n",
    "        [(5, 32), (5, 32), (1, 3)],\n",
    "    ],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "class DownBlock2D(k.Model):\n",
    "\n",
    "    def __init__(self, conv_kernels: List[tuple], lstm_kernels: List[tuple], stride=2, data_format='NCHW'):\n",
    "        super(DownBlock2D, self).__init__()\n",
    "        data_format_keras = 'channels_first' if data_format[1] == 'C' else 'channels_last'\n",
    "        channel_axis = 1 if data_format[1] == 'C' else -1\n",
    "        self.ConvLSTM = []\n",
    "        self.Conv = []\n",
    "        self.BN = []\n",
    "        self.LReLU = []\n",
    "        self.total_stride = 1\n",
    "\n",
    "        for kxy_lstm, kout_lstm in lstm_kernels:\n",
    "            self.ConvLSTM.append(k.layers.ConvLSTM2D(filters=kout_lstm, kernel_size=kxy_lstm, strides=1,\n",
    "                                                     padding='same', data_format=data_format_keras,\n",
    "                                                     return_sequences=True, stateful=True))\n",
    "\n",
    "        for l_ind, (kxy, kout) in enumerate(conv_kernels):\n",
    "            _stride = stride if l_ind == 0 else 1\n",
    "            self.total_stride *= _stride\n",
    "            self.Conv.append(k.layers.Conv2D(filters=kout, kernel_size=kxy, strides=_stride, use_bias=True,\n",
    "                                             data_format=data_format_keras, padding='same'))\n",
    "            self.BN.append(k.layers.BatchNormalization(axis=channel_axis))\n",
    "            self.LReLU.append(k.layers.LeakyReLU())\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        convlstm = inputs\n",
    "        for conv_lstm_layer in self.ConvLSTM:\n",
    "            convlstm = conv_lstm_layer(convlstm)\n",
    "\n",
    "        orig_shape = convlstm.shape\n",
    "\n",
    "        conv_input = tf.reshape(convlstm, [orig_shape[0] * orig_shape[1], orig_shape[2], orig_shape[3], orig_shape[4]])\n",
    "        activ = conv_input  # set input to for loop\n",
    "        for conv_layer, bn_layer, lrelu_layer in zip(self.Conv, self.BN, self.LReLU):\n",
    "            conv = conv_layer(activ)\n",
    "            bn = bn_layer(conv, training)\n",
    "            activ = lrelu_layer(bn)\n",
    "        out_shape = activ.shape\n",
    "        activ_down = tf.reshape(activ, [orig_shape[0], orig_shape[1], out_shape[1], out_shape[2], out_shape[3]])\n",
    "        return activ_down, activ\n",
    "\n",
    "    def reset_states_per_batch(self, is_last_batch):\n",
    "        batch_size = is_last_batch.shape[0]\n",
    "        is_last_batch = tf.reshape(is_last_batch, [batch_size, 1, 1, 1])\n",
    "        for convlstm_layer in self.ConvLSTM:\n",
    "            cur_state = convlstm_layer.states\n",
    "            new_states = (cur_state[0] * is_last_batch, cur_state[1] * is_last_batch)\n",
    "            convlstm_layer.states[0].assign(new_states[0])\n",
    "            convlstm_layer.states[1].assign(new_states[1])\n",
    "\n",
    "    def get_states(self):\n",
    "        states = []\n",
    "        for convlstm_layer in self.ConvLSTM:\n",
    "            state = convlstm_layer.states\n",
    "            states.append([s.numpy() if s is not None else s for s in state])\n",
    "\n",
    "        return states\n",
    "\n",
    "    def set_states(self, states):\n",
    "        for convlstm_layer, state in zip(self.ConvLSTM, states):\n",
    "            if None is state[0]:\n",
    "                state = None\n",
    "            convlstm_layer.reset_states(state)\n",
    "\n",
    "    @classmethod\n",
    "    def unit_test(cls):\n",
    "        conv_kernels = [(3, 16), (3, 32), (3, 64)]\n",
    "        lstm_kernels = [(3, 16), (3, 32), (3, 64)]\n",
    "        stride = 2\n",
    "        data_format = 'NHWC'  # 'NCHW'\n",
    "        training = True\n",
    "        batch_size = 2\n",
    "        unroll_len = 3\n",
    "        h = 50\n",
    "        w = 50\n",
    "        d = 3\n",
    "        model = cls(conv_kernels, lstm_kernels, stride, data_format)\n",
    "        for i in range(4):\n",
    "            input_sequence = np.random.randn(batch_size, unroll_len, h, w, d).astype(np.float32)\n",
    "\n",
    "            if data_format == 'NCHW':\n",
    "                input_sequence = np.transpose(input_sequence, (0, 4, 1, 2, 3))\n",
    "            model_out = model(input_sequence, training)\n",
    "            print(i, model_out[0].shape, model_out[1].shape)\n",
    "\n",
    "\n",
    "class UpBlock2D(k.Model):\n",
    "\n",
    "    def __init__(self, kernels: List[tuple], up_factor=2, data_format='NCHW', return_logits=False):\n",
    "        super(UpBlock2D, self).__init__()\n",
    "        self.data_format_keras = 'channels_first' if data_format[1] == 'C' else 'channels_last'\n",
    "        self.up_factor = up_factor\n",
    "        self.channel_axis = 1 if data_format[1] == 'C' else -1\n",
    "        self.Conv = []\n",
    "        self.BN = []\n",
    "        self.LReLU = []\n",
    "        self.return_logits = return_logits\n",
    "\n",
    "        for kxy, kout in kernels:\n",
    "            self.Conv.append(k.layers.Conv2D(filters=kout, kernel_size=kxy, strides=1, use_bias=True,\n",
    "                                             data_format=self.data_format_keras, padding='same'))\n",
    "\n",
    "            self.BN.append(k.layers.BatchNormalization(axis=self.channel_axis))\n",
    "            self.LReLU.append(k.layers.LeakyReLU())\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        input_sequence, skip = inputs\n",
    "        input_sequence = k.backend.resize_images(input_sequence, self.up_factor, self.up_factor, self.data_format_keras,\n",
    "                                                 interpolation='bilinear')\n",
    "        input_tensor = tf.concat([input_sequence, skip], axis=self.channel_axis)\n",
    "        for conv_layer, bn_layer, lrelu_layer in zip(self.Conv, self.BN, self.LReLU):\n",
    "            conv = conv_layer(input_tensor)\n",
    "            if self.return_logits and conv_layer == self.Conv[-1]:\n",
    "                return conv\n",
    "            bn = bn_layer(conv, training)\n",
    "            activ = lrelu_layer(bn)\n",
    "            input_tensor = activ\n",
    "        return input_tensor\n",
    "\n",
    "    @classmethod\n",
    "    def unit_test(cls):\n",
    "        conv_kernels = [(3, 16), (3, 32), (3, 64)]\n",
    "        up_factor = 2\n",
    "        data_format = 'NHWC'  # 'NCHW'\n",
    "        training = True\n",
    "        batch_size = 2\n",
    "        unroll_len = 3\n",
    "        h = 50\n",
    "        w = 50\n",
    "        d = 3\n",
    "        model = cls(conv_kernels, up_factor, data_format)\n",
    "        for i in range(4):\n",
    "            input_sequence = np.random.randn(batch_size * unroll_len, h, w, d).astype(np.float32)\n",
    "            skip = np.random.randn(batch_size * unroll_len, h * 2, w * 2, d).astype(np.float32)\n",
    "\n",
    "            if data_format == 'NCHW':\n",
    "                input_sequence = np.transpose(input_sequence, (0, 4, 1, 2, 3))\n",
    "                skip = np.transpose(skip, (0, 3, 1, 2))\n",
    "            model_out = model((input_sequence, skip), training)\n",
    "            print(i, model_out.shape)\n",
    "\n",
    "\n",
    "class ULSTMnet2D(k.Model):\n",
    "    def __init__(self, net_params=DEFAULT_NET_DOWN_PARAMS, data_format='NCHW', pad_image=True):\n",
    "        super(ULSTMnet2D, self).__init__()\n",
    "        self.data_format_keras = 'channels_first' if data_format[1] == 'C' else 'channels_last'\n",
    "        self.channel_axis = 1 if data_format[1] == 'C' else -1\n",
    "        self.DownLayers = []\n",
    "        self.UpLayers = []\n",
    "        self.total_stride = 1\n",
    "        self.pad_image = pad_image\n",
    "\n",
    "        if not len(net_params['down_conv_kernels']) == len(net_params['lstm_kernels']):\n",
    "            raise ValueError('Number of layers in down path ({}) do not match number of LSTM layers ({})'.format(\n",
    "                len(net_params['down_conv_kernels']), len(net_params['lstm_kernels'])))\n",
    "        if not len(net_params['down_conv_kernels']) == len(net_params['up_conv_kernels']):\n",
    "            raise ValueError('Number of layers in down path ({}) do not match number of layers in up path ({})'.format(\n",
    "                len(net_params['down_conv_kernels']), len(net_params['up_conv_kernels'])))\n",
    "\n",
    "        for layer_ind, (conv_filters, lstm_filters) in enumerate(zip(net_params['down_conv_kernels'],\n",
    "                                                                     net_params['lstm_kernels'])):\n",
    "            stride = 2 if layer_ind < len(net_params['down_conv_kernels']) - 1 else 1\n",
    "            self.DownLayers.append(DownBlock2D(conv_filters, lstm_filters, stride, data_format))\n",
    "            self.total_stride *= self.DownLayers[-1].total_stride\n",
    "\n",
    "        for layer_ind, conv_filters in enumerate(net_params['up_conv_kernels']):\n",
    "            up_factor = 2 if layer_ind > 0 else 1\n",
    "            self.UpLayers.append(UpBlock2D(conv_filters, up_factor, data_format,\n",
    "                                           return_logits=layer_ind + 1 == len(net_params['up_conv_kernels'])))\n",
    "            self.last_depth = conv_filters[-1][1]\n",
    "        self.Softmax = k.layers.Softmax(self.channel_axis + 1)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        input_shape = inputs.shape\n",
    "        min_pad_value = self.total_stride * int(self.pad_image) if self.pad_image else 0\n",
    "\n",
    "        if self.channel_axis == 1:\n",
    "            pad_y = [min_pad_value, min_pad_value + tf.math.mod(self.total_stride - tf.math.mod(input_shape[3],\n",
    "                                                                                                self.total_stride),\n",
    "                                                                self.total_stride)]\n",
    "            pad_x = [min_pad_value, min_pad_value + tf.math.mod(self.total_stride - tf.math.mod(input_shape[4],\n",
    "                                                                                                self.total_stride),\n",
    "                                                                self.total_stride)]\n",
    "            paddings = [[0, 0], [0, 0], [0, 0], pad_y, pad_x]\n",
    "            crops = [[0, input_shape[0]], [0, input_shape[1]], [0, self.last_depth],\n",
    "                     [pad_y[0], pad_y[0] + input_shape[3]], [pad_x[0], pad_x[0] + input_shape[4]]]\n",
    "        else:\n",
    "            pad_y = [min_pad_value, min_pad_value + tf.math.mod(self.total_stride - tf.math.mod(input_shape[2],\n",
    "                                                                                                self.total_stride),\n",
    "                                                                self.total_stride)]\n",
    "            pad_x = [min_pad_value, min_pad_value + tf.math.mod(self.total_stride - tf.math.mod(input_shape[3],\n",
    "                                                                                                self.total_stride),\n",
    "                                                                self.total_stride)]\n",
    "            paddings = [[0, 0], [0, 0], pad_y, pad_x, [0, 0]]\n",
    "            crops = [[0, input_shape[0]], [0, input_shape[1]], [pad_y[0], input_shape[2] + pad_y[0]],\n",
    "                     [pad_x[0], input_shape[3] + pad_x[0]], [0, self.last_depth]]\n",
    "        inputs = tf.pad(inputs, paddings, \"REFLECT\")\n",
    "        input_shape = inputs.shape\n",
    "\n",
    "        skip_inputs = []\n",
    "        out_down = inputs\n",
    "        out_skip = tf.reshape(inputs, [input_shape[0] * input_shape[1], input_shape[2], input_shape[3], input_shape[4]])\n",
    "        for down_layer in self.DownLayers:\n",
    "            skip_inputs.append(out_skip)\n",
    "            out_down, out_skip = down_layer(out_down, training=training, mask=mask)\n",
    "        up_input = out_skip\n",
    "        skip_inputs.reverse()\n",
    "        assert len(skip_inputs) == len(self.UpLayers)\n",
    "        for up_layer, skip_input in zip(self.UpLayers, skip_inputs):\n",
    "            up_input = up_layer((up_input, skip_input), training=training, mask=mask)\n",
    "        logits_output_shape = up_input.shape\n",
    "        logits_output = tf.reshape(up_input, [input_shape[0], input_shape[1], logits_output_shape[1],\n",
    "                                              logits_output_shape[2], logits_output_shape[3]])\n",
    "\n",
    "        logits_output = logits_output[crops[0][0]:crops[0][1], crops[1][0]:crops[1][1], crops[2][0]:crops[2][1],\n",
    "                        crops[3][0]:crops[3][1], crops[4][0]:crops[4][1]]\n",
    "        softmax_output = self.Softmax(logits_output)\n",
    "\n",
    "        return logits_output, softmax_output\n",
    "\n",
    "    @classmethod\n",
    "    def unit_test(cls):\n",
    "        net_params = DEFAULT_NET_DOWN_PARAMS\n",
    "        data_format = 'NHWC'  # 'NCHW'\n",
    "        training = True\n",
    "        pad_image = True\n",
    "        print(int(pad_image))\n",
    "\n",
    "        batch_size = 2\n",
    "        unroll_len = 2\n",
    "        h = w = 2 ** 5 + 3\n",
    "        d = 3\n",
    "        model = cls(net_params, data_format, pad_image)\n",
    "        for i in range(4):\n",
    "            input_sequence = np.random.randn(batch_size, unroll_len, h, w, d).astype(np.float32)\n",
    "\n",
    "            if data_format == 'NCHW':\n",
    "                input_sequence = np.transpose(input_sequence, (0, 4, 1, 2, 3))\n",
    "\n",
    "            model_out = model(input_sequence, training)\n",
    "\n",
    "            print(i, model_out[0].shape)\n",
    "\n",
    "    def reset_states_per_batch(self, is_last_batch):\n",
    "        for down_block in self.DownLayers:\n",
    "            down_block.reset_states_per_batch(is_last_batch)\n",
    "\n",
    "    def get_states(self):\n",
    "        states = []\n",
    "        for down_block in self.DownLayers:\n",
    "            states.append(down_block.get_states())\n",
    "        return states\n",
    "\n",
    "    def set_states(self, states):\n",
    "        for down_block, state in zip(self.DownLayers, states):\n",
    "            down_block.set_states(state)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # DownBlock2D.unit_test()\n",
    "    # UpBlock2D.unit_test()\n",
    "    ULSTMnet2D.unit_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parts of the U-Net model \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool1d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose1d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
