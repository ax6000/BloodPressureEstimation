{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dir):\n",
    "    print(os.path.isfile(dir))\n",
    "    if os.path.isfile(dir):\n",
    "        arr = np.genfromtxt(dir, dtype=str, encoding='utf-8')\n",
    "        if arr.ndim:\n",
    "            images = [i for i in arr]\n",
    "        else:\n",
    "            images = np.array([arr])\n",
    "    return images\n",
    "\n",
    "class PPG2ABPDataset_v3_base(Dataset):\n",
    "    def __init__(self,data_flist,data_root = None,\n",
    "                 data_len=1000, size=224, loader=None):\n",
    "        self.data_root = data_root\n",
    "        self.data_flist = data_flist\n",
    "        self.flist = make_dataset(self.data_flist)\n",
    "        # if data_len > 0:\n",
    "        #     self.flist = flist[:int(data_len)]\n",
    "        # else:\n",
    "        #     self.flist = flist\n",
    "        self.size = size\n",
    "        self.data=self.load_npys()\n",
    "        if data_len > 0:\n",
    "            data_index = np.arange(0,len(self.data),max(len(self.data)//int(data_len),1)).astype(int)[:int(data_len)]\n",
    "            self.data = self.data[data_index]\n",
    "        else:\n",
    "            self.data = self.data[:len(self.data)-len(self.data)%64]\n",
    "        print(\"data prepared:\" ,self.data.shape)\n",
    "    def load_npys(self):\n",
    "        data = []\n",
    "        for f in self.flist:\n",
    "            arr = np.load(self.data_root+\"\\\\\"+str(f))\n",
    "            if len(arr) != 0:\n",
    "                data.append(arr)\n",
    "        data = np.concatenate(data)\n",
    "        return data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ret = {}\n",
    "        ret['gt_image'] = np.concatenate([self.data[index,:,0][np.newaxis, :].astype(np.float32).min(axis=-1),self.data[index,:,0][np.newaxis, :].astype(np.float32).max(axis=-1)],axis=-1)\n",
    "        ret['cond_image'] = self.data[index,:,1][np.newaxis, :].astype(np.float32)\n",
    "        ret['path'] = str(index)\n",
    "        return ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "class PPG2ABPDataset_v3_Train(PPG2ABPDataset_v3_base):\n",
    "    def __init__(self, data_len=-1, size=224, loader=None, data_root=r\"..\\..\\data\\processed\\BP_npy\\1127_256_balanced\\p00\"):\n",
    "        super().__init__(data_len=data_len,data_flist = r\"..\\data\\processed\\list\\train_BP2.txt\",data_root=data_root)\n",
    "\n",
    "class PPG2ABPDataset_v3_Val(PPG2ABPDataset_v3_base):\n",
    "    def __init__(self, data_len=1000, size=224, loader=None, data_root=r\"..\\..\\data\\processed\\BP_npy\\1127_256_balanced\\p00\"):\n",
    "        super().__init__(data_len=data_len,data_flist = r\"..\\data\\processed\\list\\val_BP2.txt\",data_root=data_root)\n",
    "\n",
    "class PPG2ABPDataset_v3_Test(PPG2ABPDataset_v3_base):\n",
    "    def __init__(self, data_len=5000, size=224, loader=None, data_root=r\"..\\..\\data\\processed\\BP_npy\\1127_256_balanced\\p00\"):\n",
    "        super().__init__(data_len=data_len,data_flist = r\"..\\data\\processed\\list\\test_BP2.txt\",data_root=data_root)         \n",
    "    \n",
    "class PPG2ABPDataset_v3_Predict(PPG2ABPDataset_v3_base):\n",
    "    def __init__(self, data_len=-1,size=224, loader=None, data_root=r\"..\\..\\data\\processed\\BP_npy\\1127_256_balanced\\p00\"):\n",
    "        super().__init__(data_flist = r\"..\\data\\processed\\list\\predict_BP2.txt\",data_root=data_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"earlystoppingクラス\"\"\"\n",
    "\n",
    "    def __init__(self, path, patience=5, verbose=False):\n",
    "        \"\"\"引数：最小値の非更新数カウンタ、表示設定、モデル格納path\"\"\"\n",
    "\n",
    "        self.patience = patience    #設定ストップカウンタ\n",
    "        self.verbose = verbose      #表示の有無\n",
    "        self.counter = 0            #現在のカウンタ値\n",
    "        self.best_score = None      #ベストスコア\n",
    "        self.early_stop = False     #ストップフラグ\n",
    "        # self.val_loss_min = np.Inf   #前回のベストスコア記憶用\n",
    "        \n",
    "        self.path = path             #ベストモデル格納path\n",
    "        os.makedirs(os.path.dirname(self.path),exist_ok=True)\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        特殊(call)メソッド\n",
    "        実際に学習ループ内で最小lossを更新したか否かを計算させる部分\n",
    "        \"\"\"\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:  #1Epoch目の処理\n",
    "            self.best_score = score\n",
    "            self.checkpoint(score, model)  #記録後にモデルを保存してスコア表示する\n",
    "        elif score > self.best_score:  # ベストスコアを更新できなかった場合\n",
    "            self.counter += 1   #ストップカウンタを+1\n",
    "            if self.verbose:  #表示を有効にした場合は経過を表示\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #現在のカウンタを表示する\n",
    "                print(f\"the best of loss: {self.best_score:.5f}\")\n",
    "            if self.counter >= self.patience:  #設定カウントを上回ったらストップフラグをTrueに変更\n",
    "                self.early_stop = True\n",
    "        else:  #ベストスコアを更新した場合\n",
    "            self.checkpoint(score, model)  #モデルを保存してスコア表示\n",
    "            self.counter = 0  #ストップカウンタリセット\n",
    "\n",
    "    def checkpoint(self, score, model):\n",
    "        '''ベストスコア更新時に実行されるチェックポイント関数'''\n",
    "        if self.verbose:  #表示を有効にした場合は、前回のベストスコアからどれだけ更新したか？を表示\n",
    "            print(f'Validation loss decreased ({self.best_score:.5f} --> {score:.5f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)  #ベストモデルを指定したpathに保存\n",
    "        self.best_score = score #その時のlossを記録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# from LPCL.FIR_layer_type2 import LinearPhaseFIRLayer_type2\n",
    "# from LPCL.FIR_layer_type4 import LinearPhaseFIRLayer_type4\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, model_config, preprocess):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.preprocess = preprocess\n",
    "\n",
    "        input_shape = model_config[\"input_shape\"]\n",
    "        self.input_channels = input_shape[0]\n",
    "        \n",
    "        # if preprocess == \"LPCL\":\n",
    "        #     self.fir2_layers = nn.ModuleList([\n",
    "        #         LinearPhaseFIRLayer_type2(filter_size=32, pad_edge=\"left_right\"),\n",
    "        #         LinearPhaseFIRLayer_type2(filter_size=64, pad_edge=\"left_right\"),\n",
    "        #         LinearPhaseFIRLayer_type2(filter_size=128, pad_edge=\"left_right\"),\n",
    "        #         LinearPhaseFIRLayer_type2(filter_size=256, pad_edge=\"left_right\")\n",
    "        #     ])\n",
    "\n",
    "        #     self.fir4_layers = nn.ModuleList([\n",
    "        #         LinearPhaseFIRLayer_type4(filter_size=32, pad_edge=\"left_right\"),\n",
    "        #         LinearPhaseFIRLayer_type4(filter_size=64, pad_edge=\"left_right\"),\n",
    "        #         LinearPhaseFIRLayer_type4(filter_size=128, pad_edge=\"left_right\"),\n",
    "        #         LinearPhaseFIRLayer_type4(filter_size=256, pad_edge=\"left_right\")\n",
    "        #     ])\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.input_channels, out_channels=8, kernel_size=4, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=4, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=4, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=4, padding=2)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(256, 1)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if self.preprocess == \"LPCL\":\n",
    "        #     fir2_outputs = [layer(x) for layer in self.fir2_layers]\n",
    "        #     fir4_outputs = [layer(x) for layer in self.fir4_layers]\n",
    "        #     x = torch.cat(fir2_outputs + fir4_outputs, dim=-1)\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = F.max_pool1d(x, kernel_size=4)\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool1d(x, kernel_size=4)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool1d(x, kernel_size=4)\n",
    "\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.max_pool1d(x, kernel_size=4)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # print(\"before dense\",x.shape)\n",
    "        x = self.fc(x)  # No activation for regression\n",
    "        return x\n",
    "\n",
    "def build_cnn(model_config, preprocess):\n",
    "    model = CNN(model_config, preprocess)\n",
    "\n",
    "    learning_rate = model_config[\"learning_rate\"]\n",
    "    optimizer_params = model_config[\"optimizer_params\"]\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, **optimizer_params)\n",
    "\n",
    "    loss_fn = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "    metrics = model_config.get(\"metrics\", [])  # Placeholder for evaluation\n",
    "\n",
    "    return model, optimizer, loss_fn, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    def create_figure(gt,pred):\n",
    "      fig,ax = plt.subplots()\n",
    "      ax.plot(gt,label=\"true\")  \n",
    "      ax.plot(gt,label=\"pred\")\n",
    "      ax.legend()\n",
    "      return fig\n",
    "    def log_img(gt,pred):\n",
    "        gt, pred = gt[0].detach().clone().cpu().numpy(), pred[0].detach().clone().cpu().numpy()\n",
    "        figure = create_figure(gt,pred)\n",
    "        img = wandb.Image(figure)\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        return img\n",
    "    # Initialize Weights & Biases (wandb)\n",
    "    wandb.init(project=\"regression-training\", config=config)\n",
    "    config = wandb.config\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    train_dataset = PPG2ABPDataset_v3_Train(data_root=r\"..\\data\\processed\\BP_npy\\250107_1152\\p00\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = PPG2ABPDataset_v3_Val(data_len=-1,data_root=r\"..\\data\\processed\\BP_npy\\250107_1152\\p00\")\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "    # Model, Loss, and Optimizer\n",
    "    model = CNN(config, \"not\")\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "    mae = nn.L1Loss()  # Mean Squared Error Loss for regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    earlystopping = EarlyStopping(f\"{config.output_path}/best.pth\",config.patience,verbose=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    wandb.watch(model, log_freq=config.log_interval)\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_loss_mae = 0.0\n",
    "        # Training phase with progress bar\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{config.epochs} - Training\", leave=False)\n",
    "        for batch_idx,data in enumerate(train_loader_tqdm):\n",
    "            gt = data[\"gt_image\"]\n",
    "            cond = data[\"cond_image\"]\n",
    "            gt,cond = gt.to(device), cond.to(device)\n",
    "            # print(gt.shape,cond.shape)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cond)\n",
    "            loss = criterion(outputs, gt)\n",
    "            loss_mae = mae(outputs, gt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # if batch_idx % config.log_interval == 0:\n",
    "                # wandb.log({\"loss\": loss})\n",
    "            # if batch_idx  == 0:\n",
    "                # wandb.log({\"train/loss\": log_img(gt,outputs)})\n",
    "            running_loss += loss.item()\n",
    "            running_loss_mae += loss_mae.item()\n",
    "            train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_loss_mae = running_loss_mae / len(train_loader)\n",
    "\n",
    "        # Validation phase with progress bar\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_loss_mae = 0.0\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{config.epochs} - Validation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx,data in enumerate(val_loader_tqdm):\n",
    "                gt = data[\"gt_image\"]\n",
    "                cond = data[\"cond_image\"]\n",
    "                gt,cond = gt.to(device), cond.to(device)\n",
    "                outputs = model(cond)\n",
    "                loss = criterion(outputs, gt)\n",
    "                loss_mae = mae(outputs, gt)\n",
    "                # if batch_idx % config.log_interval == 0:\n",
    "                    # wandb.log({\"loss\": loss})\n",
    "                # if batch_idx  == 0:\n",
    "                    # wandb.log({\"val/loss\": log_img(gt,outputs)})\n",
    "                val_loss += loss.item()\n",
    "                val_loss_mae += loss_mae.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_loss_mae /= len(val_loader)\n",
    "        earlystopping(val_loss,model)\n",
    "        if earlystopping.early_stop:\n",
    "            print(\"Early Stopping!\")\n",
    "            break\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train/loss_epoch\": train_loss,\n",
    "            \"train/mae\": train_loss_mae,\n",
    "            \"val/loss_epoch\": val_loss,\n",
    "            \"val/mae\": val_loss_mae\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{config.epochs}]\"\n",
    "              f\" Train Loss: {train_loss:.4f}\"\n",
    "              f\" Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: 24amj35 (bsa_mh). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.19.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\minowa\\BloodPressureEstimation\\notebooks\\wandb\\run-20250108_182131-57kswwbu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bsa_mh/regression-training/runs/57kswwbu' target=\"_blank\">dazzling-firefly-36</a></strong> to <a href='https://wandb.ai/bsa_mh/regression-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bsa_mh/regression-training' target=\"_blank\">https://wandb.ai/bsa_mh/regression-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bsa_mh/regression-training/runs/57kswwbu' target=\"_blank\">https://wandb.ai/bsa_mh/regression-training/runs/57kswwbu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "data prepared: (102144, 1152, 2)\n",
      "True\n",
      "data prepared: (12160, 1152, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.01612 --> 0.01612).  Saving model ...\n",
      "Epoch [1/100] Train Loss: 0.0134 Val Loss: 0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 10\n",
      "the best of loss: 0.01612\n",
      "Epoch [2/100] Train Loss: 0.0107 Val Loss: 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 10\n",
      "the best of loss: 0.01612\n",
      "Epoch [3/100] Train Loss: 0.0099 Val Loss: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.01612 --> 0.01589).  Saving model ...\n",
      "Epoch [4/100] Train Loss: 0.0093 Val Loss: 0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.01589 --> 0.01562).  Saving model ...\n",
      "Epoch [5/100] Train Loss: 0.0089 Val Loss: 0.0156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.01562 --> 0.01510).  Saving model ...\n",
      "Epoch [6/100] Train Loss: 0.0085 Val Loss: 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 10\n",
      "the best of loss: 0.01510\n",
      "Epoch [7/100] Train Loss: 0.0082 Val Loss: 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 10\n",
      "the best of loss: 0.01510\n",
      "Epoch [8/100] Train Loss: 0.0080 Val Loss: 0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 10\n",
      "the best of loss: 0.01510\n",
      "Epoch [9/100] Train Loss: 0.0078 Val Loss: 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 10\n",
      "the best of loss: 0.01510\n",
      "Epoch [10/100] Train Loss: 0.0077 Val Loss: 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 10\n",
      "the best of loss: 0.01510\n",
      "Epoch [11/100] Train Loss: 0.0075 Val Loss: 0.0163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 6 out of 10\n",
      "the best of loss: 0.01510\n",
      "Epoch [12/100] Train Loss: 0.0074 Val Loss: 0.0175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 7 out of 10\n",
      "the best of loss: 0.01510\n",
      "Epoch [13/100] Train Loss: 0.0073 Val Loss: 0.0163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 8 out of 10\n",
      "the best of loss: 0.01510\n",
      "Epoch [14/100] Train Loss: 0.0072 Val Loss: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 9 out of 10\n",
      "the best of loss: 0.01510\n",
      "Epoch [15/100] Train Loss: 0.0071 Val Loss: 0.0173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 10 out of 10\n",
      "the best of loss: 0.01510\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\":100,\n",
    "    \"log_interval\":100,\n",
    "    \"input_shape\":[1,1152],\n",
    "    \"output_path\": \"outputs\\\\cnn\\\\0108_2\",\n",
    "    \"patience\":10    \n",
    "          }\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\":100,\n",
    "    \"log_interval\":100,\n",
    "    \"input_shape\":[1,1152],\n",
    "    \"output_path\": \"outputs\\\\cnn\\\\0108\",\n",
    "    \"patience\":10    \n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_ckpt = \".\\\\\"+config[\"output_path\"]+\"\\\\best.pth\"\n",
    "model = CNN(config, \"not\").to(device)\n",
    "model.load_state_dict(torch.load(best_ckpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "data prepared: (21696, 1152, 2)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = PPG2ABPDataset_v3_Test(data_len=-1,data_root=r\"..\\data\\processed\\BP_npy\\250107_1152\\p00\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = []\n",
    "gt = []\n",
    "\n",
    "for data in test_loader:\n",
    "    _gt = data[\"gt_image\"]\n",
    "    cond = data[\"cond_image\"]\n",
    "    cond = cond.to(device)\n",
    "    output.append(model(cond).detach().cpu().numpy())\n",
    "    gt.append(_gt.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.concatenate(output)\n",
    "gt = np.concatenate(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21696, 1), (21696, 2))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape,gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = np.load(r\"../data\\processed\\BP_npy\\250107_1152\\p00\\scale_train.npy\")\n",
    "gt[:] -= scales[0,0]\n",
    "gt[:] /= scales[0,1]\n",
    "output[:] -= scales[0,0]\n",
    "output[:] /= scales[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21696, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() () () ()\n"
     ]
    }
   ],
   "source": [
    "errors = output-gt[:,0]\n",
    "me = np.mean(errors)\n",
    "mae = np.mean(np.abs(errors))\n",
    "rmse = np.sqrt(np.mean(errors**2))\n",
    "std = np.std(errors)\n",
    "print(me.shape,mae.shape,rmse.shape,std.shape)\n",
    "n_samples = errors.shape[0]\n",
    "\n",
    "error_5 = np.count_nonzero(np.abs(errors)<=5)/n_samples*100\n",
    "error_10 = np.count_nonzero(np.abs(errors)<=10)/n_samples*100\n",
    "error_15 = np.count_nonzero(np.abs(errors)<=15)/n_samples*100\n",
    "# me = np.mean(errors,axis=0)\n",
    "# mae = np.mean(np.abs(errors),axis=0)\n",
    "# rmse = np.sqrt(np.mean(errors**2,axis=0))\n",
    "# std = np.std(errors,axis=0)\n",
    "# print(me.shape,mae.shape,rmse.shape,std.shape)\n",
    "# n_samples = errors.shape[0]\n",
    "\n",
    "# error_5 = np.count_nonzero(np.abs(errors)<=5,axis=0)/n_samples*100\n",
    "# error_15 = np.count_nonzero(np.abs(errors)<=15,axis=0)/n_samples*100\n",
    "# error_10 = np.count_nonzero(np.abs(errors)<=10,axis=0)/n_samples*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.727077"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          test data samples:\n",
      "          # samples : 21696\n",
      "          \n",
      "          Eval Stats:   DBP    SBP\n",
      "          MAE:         8.070 -\n",
      "          RMSE:       10.174 -\n",
      "          Mean Error: -0.232 -\n",
      "          STD:        10.171 -\n",
      "          \n",
      "          BHS standards range:\n",
      "          Error   <5mmHg <10mmHg <15mmHg\n",
      "          gradeA     60%     85%     95%\n",
      "          gradeB     50%     75%     90%\n",
      "          gradeC     40%     65%     85%\n",
      "          DBP     797441.8%  1460944.2%  1917213.7%\n",
      "           \n",
      "          \n",
      "          \n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "          test data samples:\n",
    "          # samples : {}\n",
    "          \n",
    "          Eval Stats:   DBP    SBP\n",
    "          MAE:        {:6.3f} -\n",
    "          RMSE:       {:6.3f} -\n",
    "          Mean Error: {:6.3f} -\n",
    "          STD:        {:6.3f} -\n",
    "          \n",
    "          BHS standards range:\n",
    "          Error   <5mmHg <10mmHg <15mmHg\n",
    "          gradeA     60%     85%     95%\n",
    "          gradeB     50%     75%     90%\n",
    "          gradeC     40%     65%     85%\n",
    "          DBP     {:5.1f}%  {:5.1f}%  {:5.1f}%\n",
    "           \n",
    "          \n",
    "          \"\"\".format(\n",
    "            n_samples,\n",
    "            mae,\n",
    "            rmse,\n",
    "            me,\n",
    "            std,\n",
    "            error_5, error_10, error_15,\n",
    "            # error_5[1], error_10[1], error_15[1],\n",
    "          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          test data samples:\n",
      "          # samples : 21696\n",
      "          \n",
      "          Eval Stats:   DBP    SBP\n",
      "          MAE:         7.450 16.194\n",
      "          RMSE:        9.661 19.907\n",
      "          Mean Error: -1.356  2.898\n",
      "          STD:         9.565 19.695\n",
      "          \n",
      "          BHS standards range:\n",
      "          Error   <5mmHg <10mmHg <15mmHg\n",
      "          gradeA     60%     85%     95%\n",
      "          gradeB     50%     75%     90%\n",
      "          gradeC     40%     65%     85%\n",
      "          DBP      41.7%   73.7%   89.4%\n",
      "          SBP      19.0%   36.0%   51.8%\n",
      "           \n",
      "          \n",
      "          \n",
      "## string for google spredsheet ##\n",
      "7.450,9.661,-1.356,9.565,16.194,19.907,2.898,19.695\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "          test data samples:\n",
    "          # samples : {}\n",
    "          \n",
    "          Eval Stats:   DBP    SBP\n",
    "          MAE:        {:6.3f} {:6.3f}\n",
    "          RMSE:       {:6.3f} {:6.3f}\n",
    "          Mean Error: {:6.3f} {:6.3f}\n",
    "          STD:        {:6.3f} {:6.3f}\n",
    "          \n",
    "          BHS standards range:\n",
    "          Error   <5mmHg <10mmHg <15mmHg\n",
    "          gradeA     60%     85%     95%\n",
    "          gradeB     50%     75%     90%\n",
    "          gradeC     40%     65%     85%\n",
    "          DBP     {:5.1f}%  {:5.1f}%  {:5.1f}%\n",
    "          SBP     {:5.1f}%  {:5.1f}%  {:5.1f}%\n",
    "           \n",
    "          \n",
    "          \"\"\".format(\n",
    "            n_samples,\n",
    "            *mae,\n",
    "            *rmse,\n",
    "            *me,\n",
    "            *std,\n",
    "            error_5[0], error_10[0], error_15[0],\n",
    "            error_5[1], error_10[1], error_15[1],\n",
    "          ))\n",
    "print(\"## string for google spredsheet ##\")\n",
    "print(\",\".join(f\"{x:.3f}\" for x in[mae[0],rmse[0],me[0],std[0],mae[1],rmse[1],me[1],std[1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
